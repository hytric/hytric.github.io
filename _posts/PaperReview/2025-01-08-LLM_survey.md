---
title: "[ë…¼ë¬¸ë¶„ì„] A Survey of Large Language Models"
last_modified_at: 2025-1-8
categories:
  - PaperReview
excerpt: "Pre-training, Adaptation, Utilization, Capability Evaluation ì— ëŒ€í•œ ë‚´ìš© íƒêµ¬"
use_math: true
classes: wide
---

> Pre-training, Adaptation, Utilization, Capability Evaluation ì— ëŒ€í•œ ë‚´ìš© íƒêµ¬
[[Paper](https://arxiv.org/abs/2303.18223)]
>   

 
<br>


<div style="padding: 10px; margin: 10px 0; border-radius: 5px; text-align: left; background: rgb(233, 243, 248); box-sizing: border-box;">
  <h1 id="index" style="margin: 0;">
    survey paper index
    <a href="#index" style="text-decoration: none; color: inherit;"></a>
  </h1>
</div>


ì´ë²ˆ ë‚´ìš©

- **Section 3:** LLMsì˜ ë°°ê²½ê³¼ GPT ì‹œë¦¬ì¦ˆ ëª¨ë¸ì˜ ë°œì „ ê³¼ì •

---

ë‹¤ìŒ ë‚´ìš©

- **Section 4:** LLMs ê°œë°œì„ ìœ„í•œ ì´ìš© ê°€ëŠ¥í•œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
- **Sections 5â€“8:** ì‚¬ì „ í›ˆë ¨, ì ì‘, í™œìš©, ëŠ¥ë ¥ í‰ê°€ì— ëŒ€í•œ ìµœê·¼ ì—°êµ¬ ë™í–¥ ë¦¬ë·°
- **Section 9:** í”„ë¡¬í”„íŠ¸ ì„¤ê³„(Prompt Design)ë¥¼ ìœ„í•œ ì‹¤ìš©ì  ê°€ì´ë“œ
- **Section 10:** ëŒ€í‘œì ì¸ ë„ë©”ì¸ì—ì„œ LLMsì˜ í™œìš© ì‚¬ë¡€
- **Section 11:** ì£¼ìš” ë°œê²¬ ì‚¬í•­ ìš”ì•½ ë° ë¯¸ë˜ ì—°êµ¬ ë°©í–¥ ì œì‹œ

 
<br>


<div style="padding: 10px; margin: 10px 0; border-radius: 5px; text-align: left; background: rgb(233, 243, 248); box-sizing: border-box;">
  <h1 id="Abstract" style="margin: 0;">
    1 Abstract
    <a href="#Abstract" style="text-decoration: none; color: inherit;"></a>
  </h1>
</div>



LLMì˜ ë“±ì¥ ë°°ê²½, ì£¼ìš” ì—°êµ¬ ê²°ê³¼, ì£¼ìš” ê¸°ìˆ ì„ ì†Œê°œí•¨ìœ¼ë¡œì¨ ìµœê·¼ì˜ ë°œì „ ìƒí™© í™•ì¸

pre-training, adaptation tuning, utilization, and capacity evaluation : LLMì˜ ë„¤ ê°€ì§€ ì£¼ìš” ì¸¡ë©´ì— ì´ˆì 

LLMì„ ê°œë°œí•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë¦¬ì†ŒìŠ¤ë¥¼ ìš”ì•½, í–¥í›„ ë°©í–¥ì— ëŒ€í•œ ë‚¨ì€ ê³¼ì œì— ëŒ€í•´ì„œë„ ë…¼ì˜

<br>

<div style="padding: 10px; margin: 10px 0; border-radius: 5px; text-align: left; background: rgb(233, 243, 248); box-sizing: border-box;">
  <h1 id="Introduction" style="margin: 0;">
    2 Introduction
    <a href="#Introduction" style="text-decoration: none; color: inherit;"></a>
  </h1>
</div>



> â€œThe limits of my language mean the limits of my world.â€
â€”Ludwig Wittgenstein
> 

LLMì€ ë¯¸ë˜ í˜¹ì€ ë¹„ì–´ìˆëŠ” í† í°ì„ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµ ë¨

### 2.1 Statistical language models (SLM)

---

1990së…„ëŒ€ ê°œë°œëœ í†µê³„ì  í•™ìŠµ ê¸°ë²• ê¸°ë°˜ + Markov assumptionì— ê¸°ë°˜í•œ ë‹¨ì–´ ì˜ˆì¸¡ ëª¨ë¸ â‡’ n-gram language models ì´ë¼ ë¶€ë¦„

í•˜ì§€ë§Œ ì°¨ì›ì´ ë†’ì•„ì§€ë©´ step ìˆ˜ ì¦ê°€ â†’  curse of dimensionality ë°œìƒ

smoothing strategies ë°©ë²• ë“±ì¥ (backoff estimation, Goodâ€“Turing estimation) â†’ data sparsity problem ì™„í™”

íŠ¹ì •  taskì— ë„ì›€ì„ ì£¼ëŠ” ì—­í• 

### 2.2 Neural language models (NLM)

---

multi-layer perceptron (MLP) ë° recurrent neural networks (RNNs)ê³¼ ê°™ì€ ì‹ ê²½ë§ì— ì˜í•´ ë‹¨ì–´ ì‹œí€€ìŠ¤ì˜ í™•ë¥ ì„ characterize

distributed representation of words + aggregated context features (i.e., the distributed word vectors) â†’  the word prediction function

ë‹¨ì–´ í‘œí˜„ í•™ìŠµ ë°©ë²•ìœ¼ë¡œ word2vec ì œì•ˆë¨

íŠ¹ì • taskì— ì¢…ì†ë˜ì§€ ì•ŠëŠ”(Task-agnostic) ì¼ë°˜ì  íŠ¹ì§• ë²¡í„°ë¥¼ í•™ìŠµ

### 2.3 Pre-trained language models (PLM)

---

ELMo : pre-training a bidirectional LSTM (biLSTM) network ì‚¬ìš© â†’ fine-tuning the biLSTM network = specific downstream tasks

 self-attention mechanismsì„ ë„ì…í•œ highly parallelizable Transformer architecture â†’ BERT (pre-training bidirectional language models) ì´ëŠ” large-scale unlabeled corporaì—ì„œ ë™ì‘, ë²”ìš©ì 

â€œpre-training and fine-tuningâ€ learning paradigmì´ ë¨ (GPT-2, BERT ë“±ë“±ì´ ê°œë°œë¨)

Contextë¥¼ ì´í•´ â†’ ì§ˆì˜ì‘ë‹µ, ê°ì„± ë¶„ë¥˜ task

### 2.4 Large language models (LLM)

---

PLMì„ í™•ì¥ (e.g., scaling model size or data size) â†’ downstream tasksì—ì„œ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ

ëŒ€í™” ì„±ëŠ¥ í–¥ìƒ (few-shot tasks)

General-purpose Task Solvers â†’ ë‹¤ì–‘í•œ ì‘ì—… ê°€ëŠ¥

task solving capacity

![image.png](/assets/Images/2025-01-08-LLM_survey/image.png)

### 2.5 AGI(Artificial General Intelligence)

ChatGPTì™€ GPT-4 â†’ ì¸ê°„ ìˆ˜ì¤€ í˜¹ì€ ê·¸ ì´ìƒì˜ ì§€ëŠ¥ì„ êµ¬í˜„

- ìì—°ì–´ ì²˜ë¦¬(NLP): NLP ì—°êµ¬ì—ì„œ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ í™œìš© ë° LLMs ì¤‘ì‹¬ì˜ ì—°êµ¬ê°€ ì£¼ë¥˜
- ì •ë³´ ê²€ìƒ‰(IR): ì „í†µì ì¸ ê²€ìƒ‰ ì—”ì§„ ë°©ì‹ â†’ ëŒ€í™”í˜• AI(ì˜ˆ: ChatGPT)
- ì»´í“¨í„° ë¹„ì „(CV): ì‹œê° ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ë©´ì„œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ëŒ€í™” ê¸°ëŠ¥ê¹Œì§€ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ”, ChatGPT ìŠ¤íƒ€ì¼ì˜ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ê°œë°œ

### Emergent Abilities

ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLMs)ì´ ì¼ì • ê·œëª¨(íŒŒë¼ë¯¸í„° ìˆ˜, ë°ì´í„° í¬ê¸° ë“±)ì— ë„ë‹¬í–ˆì„ ë•Œ **ì˜ˆìƒì¹˜ ëª»í•˜ê²Œ ë‚˜íƒ€ë‚˜ëŠ” ìƒˆë¡œìš´ ëŠ¥ë ¥ì´ë‚˜ í–‰ë™** â†’ ì•„ë˜ì„œ ìì„¸í•˜ê²Œ ë‹¤ë£° ì˜ˆ

ì‘ì€ PLMs(Pre-trained Language Models)ì—ì„œëŠ” ë‚˜íƒ€ë‚˜ì§€ ì•Šê³ , LLMsì—ì„œë§Œ ë°œìƒ

### training ê³¼ì •ì˜ ë¶ˆíˆ¬ëª…ì„± ë° ë†’ì€ ë¹„ìš©

### Alignment with Human Values

Toxic, Fictitious, Harmful ì½˜í…ì¸  ìƒì„±ì— ëŒ€í•œ ë¬¸ì œ

<br>

<div style="padding: 10px; margin: 10px 0; border-radius: 5px; text-align: left; background: rgb(233, 243, 248); box-sizing: border-box;">
  <h1 id="Overview" style="margin: 0;">
    3 Overview
    <a href="#Overview" style="text-decoration: none; color: inherit;"></a>
  </h1>
</div>


## **3.1 Background for LLMs**

hundreds of billions (or more) of parameters

### 3.1.1 Formulation of Scaling Laws for LLMs

---

**ì–¸ì–´ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ëª¨ë¸ í¬ê¸°, ë°ì´í„° í¬ê¸°, ê³„ì‚°ëŸ‰ê³¼ ì–´ë–»ê²Œ ìƒí˜¸ì‘ìš© í•˜ëŠ”ì§€**ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì„¤ëª…í•˜ëŠ” ê²½í—˜ì  ê·œì¹™

LLMs(ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸)ê°€ ë” ë§ì€ ë¦¬ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í• ìˆ˜ë¡ ì„±ëŠ¥ì´ ì–´ë–»ê²Œ í–¥ìƒ ë˜ëŠ”ì§€ë¥¼ ë³´ì—¬ì¤Œ

LLMì€ Transformer architectureë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•¨ â†’ Multi-Head Attentionì„ í†µí•´ Contextë¥¼ íŒŒì•…

ì´ˆê¸° ì†Œí˜• ì–¸ì–´ ëª¨ë¸ê³¼ LLMsëŠ” ìœ ì‚¬í•œ Pre-training Objectiveë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, LLMsëŠ” ëª¨ë¸ í¬ê¸°(Model Size, N), ë°ì´í„° í¬ê¸°(Data Size, D), ê³„ì‚°ëŸ‰(Compute, C)ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ í™•ì¥

ë”°ë¼ì„œ ëª¨ë¸ ì„±ëŠ¥ê³¼ ìì› í• ë‹¹ ìµœì í™”ë¥¼ ìœ„í•œ Scaling Lawsê°€ ì—°êµ¬ë¨

### **KM Scaling Law (Kaplan et al., 2020, OpenAI)**

---

- LLMsì˜ **ì„±ëŠ¥ê³¼ 3ê°€ì§€ ì£¼ìš” ìš”ì†Œ**(ëª¨ë¸ í¬ê¸°, ë°ì´í„° í¬ê¸°, ê³„ì‚°ëŸ‰) ê°„ì˜ ë©±ë²•ì¹™(Power-Law Relationship)ì„ ë°œê²¬
- ì£¼ì–´ì§„ ê³„ì‚° ì˜ˆì‚°(compute budget, C) í•˜ì—ì„œ ì–¸ì–´ ëª¨ë¸ì˜ ì†ì‹¤(Language Modeling Loss, L)ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„
    
    $$
    
    L(N) = \left(\frac{N_c}{N}\right)^{\alpha_N}, \quad \alpha_N \approx 0.076, \quad N_c \approx 8.8 \times 10^{13}
    
    $$
    
    $$
    L(D) = \left(\frac{D_c}{D}\right)^{\alpha_D}, \quad \alpha_D \approx 0.095, \quad D_c \approx 5.4 \times 10^{13}
    $$
    
    $$
    
    L(C) = \left(\frac{C_c}{C}\right)^{\alpha_C}, \quad \alpha_C \approx 0.050, \quad C_c \approx 3.1 \times 10^{8}
    
    $$
    
    - L(â‹…): ì–¸ì–´ ëª¨ë¸ì˜ ì†ì‹¤ (Cross-Entropy Loss in nats)
    - N: ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ (Model Size)
    - D: í•™ìŠµ ë°ì´í„° í† í° ìˆ˜ (Dataset Size)
    - C: í•™ìŠµ ê³„ì‚°ëŸ‰ (Compute)
    - $$\alpha_N, \alpha_D, \alpha_C$$ : ê° ìš”ì†Œì˜ ì†ì‹¤ ê°ì†Œìœ¨ (Loss Decay Rate)
    - $$ N_c, D_c, C_c $$ : ìµœì í™” ì„ê³„ê°’ (Optimal Threshold Values)

- **ëª¨ë¸ ì„±ëŠ¥ì€ ëª¨ë¸ í¬ê¸°(N), ë°ì´í„° í¬ê¸°(D), ê³„ì‚°ëŸ‰(C)ì— ë”°ë¼ ë©±ë²•ì¹™ì ìœ¼ë¡œ ë³€í™”**
- ì†ì‹¤ì€ ëª¨ë¸ì˜ í¬ê¸°, ë°ì´í„° ì–‘, ê³„ì‚°ëŸ‰ì„ ëŠ˜ë¦´ìˆ˜ë¡ ê°ì†Œí•˜ì§€ë§Œ, ê·¸ ë¹„ìœ¨ì€ ì ì°¨ ì¤„ì–´ë“¬ (ìˆ˜ìµ ì²´ê°ì˜ ë²•ì¹™, Diminishing Returns).
- ì´ ë²•ì¹™ì€ ë‹¤ì–‘í•œ ë²”ìœ„ì˜ ëª¨ë¸ í¬ê¸°(768 ~ 1.5B íŒŒë¼ë¯¸í„°), ë°ì´í„°(22M ~ 23B í† í°), ê³„ì‚°ëŸ‰ì„ í†µí•´ ì‹¤í—˜ì ìœ¼ë¡œ ê²€ì¦

- ëŒ€ê·œëª¨ ì‹¤í—˜ê³¼ ë°ì´í„° ë¶„ì„ì„ í†µí•´ ê²½í—˜ì ìœ¼ë¡œ ê²€ì¦ëœ ê´€ê³„ â†’ ìˆ˜ì‹ì ìœ¼ë¡œ ì¦ëª… ì•ˆë¨

<br>

ğŸ“Œ **ì–´ë–»ê²Œ ì¦ëª…í•˜ëŠ”ê°€?**

âœ… **ì£¼ìš” ë³€ìˆ˜**

1. **ëª¨ë¸ í¬ê¸° (Model Size, N)**
    - íŒŒë¼ë¯¸í„° ìˆ˜: 768 ~ 1.5B (15ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°)
2. **ë°ì´í„° í¬ê¸° (Dataset Size, D)**
    - í•™ìŠµì— ì‚¬ìš©ëœ í† í° ìˆ˜: 22M (2200ë§Œ ê°œ) ~ 23B (230ì–µ ê°œ)
3. **ê³„ì‚°ëŸ‰ (Compute, C)**
    - ì´ í•™ìŠµ ê³„ì‚°ëŸ‰: ë‹¤ì–‘í•œ í•™ìŠµ ìŠ¤í…ê³¼ GPU ì‹œê°„ ì¡°í•©

âœ… **í‰ê°€ ì§€í‘œ**

- **Cross-Entropy Loss (L)**: ì–¸ì–´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ì£¼ìš” ì†ì‹¤ í•¨ìˆ˜
- **Scaling Behavior**: ê° ë³€ìˆ˜(N, D, C)ì˜ ë³€í™”ì— ë”°ë¥¸ Loss ê°ì†Œìœ¨ì„ ë¶„ì„

âœ… **ë°©ë²•**

ê° ì‹¤í—˜ì—ì„œ í•œ ê°€ì§€ ë³€ìˆ˜ë§Œ ë³€í™” â†’ ë¡œê·¸-ë¡œê·¸ ìŠ¤ì¼€ì¼ í”Œë¡¯ (Log-Log Scale Plotting) â†’ ì§ì„ ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ë©´ Power-Law Relationship ì„±ë¦½

<br>

ğŸ“Œ **Power-Law Relationshipì´ë€?**

$$
y=kx^Î±
$$

ë‹¤ìŒê³¼ ê°™ì€ ê´€ê³„ë¥¼ ë§Œì¡±í•˜ëŠ” ê²½ìš° â†’ ì–´ë–¤ í˜„ìƒì´ ê±°ë“­ ì œê³±ì˜ íŒ¨í„´ì„ ë”°ë¥´ëŠ” ê²ƒ

- y: ì¢…ì† ë³€ìˆ˜ (Dependent Variable)
- x: ë…ë¦½ ë³€ìˆ˜ (Independent Variable)
- k: ë¹„ë¡€ ìƒìˆ˜ (Proportionality Constant)
- Î±: ë©±ì§€ìˆ˜(Exponent) â€“ ê´€ê³„ì˜ ê¸°ìš¸ê¸° ë˜ëŠ” ë¹„ìœ¨

![image.png](/assets/Images/2025-01-08-LLM_survey/image1.png)

íŠ¹ì§•

- **ë¹„ì„ í˜•ì  ê´€ê³„ (Non-linear Relationship)**
    - ë…ë¦½ ë³€ìˆ˜ xì˜ ì¦ê°€ì— ë”°ë¼ ì¢…ì† ë³€ìˆ˜ yê°€ ì„ í˜•ì (ì¼ì •í•œ ë¹„ìœ¨)ìœ¼ë¡œ ì¦ê°€í•˜ì§€ ì•Šê³ , ë©±ì§€ìˆ˜(Î±)ì— ë”°ë¼ ê¸‰ê²©í•˜ê²Œ ë³€í™”
- **ìê¸°ìœ ì‚¬ì„± (Self-Similarity)**
    - íŠ¹ì • ë²”ìœ„ì—ì„œ ê´€ì°°ëœ íŒ¨í„´ì´ ë‹¤ë¥¸ ë²”ìœ„ì—ì„œë„ ë°˜ë³µ. (í”„ë™íƒˆ êµ¬ì¡°ì™€ ìœ ì‚¬í•¨)
- **ìˆ˜ìµ ì²´ê°ì˜ ë²•ì¹™ (Diminishing Returns)**
    - ë©±ì§€ìˆ˜ Î±<1ì¸ ê²½ìš°, xê°€ ì¦ê°€í• ìˆ˜ë¡ yì˜ ì¦ê°€ìœ¨ì´ ì ì°¨ ê°ì†Œ
- **ê¸´ ê¼¬ë¦¬ ë¶„í¬ (Long Tail Distribution)**
    - ì¼ë¶€ í•­ëª©ì´ ë§¤ìš° í° ê°’ì„ ê°€ì§€ëŠ” ë°˜ë©´, ëŒ€ë¶€ë¶„ì˜ í•­ëª©ì€ ì‘ì€ ê°’ (ì˜ˆ: ì†Œìˆ˜ì˜ ìœ ëª… ìœ íŠœë²„ê°€ ì—„ì²­ë‚œ ì¡°íšŒìˆ˜ë¥¼ ê¸°ë¡í•˜ëŠ” í˜„ìƒ)

í•˜ì§€ë§Œ ìœ„ ë°©ë²• ë°ì´í„° ë¶€ì¡± ë¬¸ì œë¥¼ ê°„ê³¼

### **Chinchilla Scaling Law (Hoffmann et al., 2022, DeepMind)**

---

KM Scaling Lawì˜ í•œê³„ë¥¼ ê·¹ë³µ

ëª¨ë¸ í¬ê¸°(N)ì™€ **ë°ì´í„° í¬ê¸°(D)** ê°„ì˜ ê· í˜•ì„ ê°•ì¡° â†’ ìµœì ì˜ ê³„ì‚° cost ë°°ë¶„ ë°©ì‹ì„ ì œì‹œ

$$
L(N, D) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta}
$$

- **L(N, D)**: ëª¨ë¸ ì†ì‹¤
- **E**: ë°ì´í„°ì˜ ê·¼ë³¸ì ì¸ ì†ì‹¤(irreducible loss, ì—”íŠ¸ë¡œí”¼)
- **A, B**: ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜(scaling coefficients)
- **Î±, Î²**: ê°ê° ëª¨ë¸ í¬ê¸°ì™€ ë°ì´í„° í¬ê¸°ì— ëŒ€í•œ ì†ì‹¤ ê°ì†Œìœ¨
    - **Î± = 0.34**
    - **Î² = 0.28**

$$
N_{\text{opt}}(C) = G \left(\frac{C}{6}\right)^a, \quad D_{\text{opt}}(C) = G^{-1} \left(\frac{C}{6}\right)^b
$$

- **G**: Scaling Coefficient, (A, B, Î±, Î²ë¡œë¶€í„° ê³„ì‚°ë¨)

$$
a = \frac{\alpha}{\alpha + \beta}, \quad b = \frac{\beta}{\alpha + \beta}
$$

**ê³„ì‚°ëŸ‰ì„ ëª¨ë¸ í¬ê¸°(N)ì™€ ë°ì´í„° í¬ê¸°(D)ì— ê· ë“±í•˜ê²Œ ë¶„ë°°**í•´ì•¼ í•œë‹¤ê³  ì£¼ì¥

**KM Scaling Law**ì™€ ë‹¬ë¦¬, ë°ì´í„° ë¶€ì¡± í˜„ìƒì„ í•´ê²°í•˜ê³  ëª¨ë¸ì´ ì¶©ë¶„í•œ ë°ì´í„°ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ê°•ì¡°

ì´ ë²•ì¹™ì„ ë”°ë¥´ë©´, ê°™ì€ ê³„ì‚° ì˜ˆì‚° í•˜ì—ì„œ **ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‹¬ì„±**

| **ë¹„êµ í•­ëª©** | **KM Scaling Law** | **Chinchilla Scaling Law** |
| --- | --- | --- |
| **ì¤‘ì  ìš”ì†Œ** | ëª¨ë¸ í¬ê¸°(Model Size)ì— ì§‘ì¤‘ | ëª¨ë¸ í¬ê¸°ì™€ ë°ì´í„° í¬ê¸°ì˜ ê· í˜• |
| **ê³„ì‚° ì˜ˆì‚° ë¶„ë°°** | ëª¨ë¸ í¬ê¸°ì— ë” ë§ì´ í• ë‹¹ | ëª¨ë¸ í¬ê¸°ì™€ ë°ì´í„° í¬ê¸°ì— ê· ë“± ë¶„ë°° |
| **ë°ì´í„° ì¤‘ìš”ì„±** | ìƒëŒ€ì ìœ¼ë¡œ ê³¼ì†Œí‰ê°€ | ê· ë“±í•œ ì¤‘ìš”ì„± ë¶€ì—¬ |
| **ì‹¤í—˜ ë²”ìœ„** | ì¤‘ì†Œí˜• ëª¨ë¸ ì¤‘ì‹¬ | ë” í° ë²”ìœ„ì˜ ëª¨ë¸ ì‹¤í—˜ |

### 3.1.2 Discussion on Scaling Laws

---

**â—¼ï¸Predictable Scaling**

**ì‘ì€ ëª¨ë¸ì˜ ì„±ëŠ¥ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë” í° ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì˜ˆì¸¡**í•˜ëŠ” ê²ƒ

âœ…  ì´ì 

- Knowledge Transfer from Small Models : ëŒ€ê·œëª¨ ëª¨ë¸ì€ **ì‹œê°„ê³¼ ë¦¬ì†ŒìŠ¤ê°€ ë§ì´ ì†Œëª¨ë˜ê¸° ë•Œë¬¸ì—** ë‹¤ì–‘í•œ í•™ìŠµ ì „ëµ(ì˜ˆ: ë°ì´í„° í˜¼í•© ë¹„ìœ¨ ì¡°ì •, í•™ìŠµ ìŠ¤ì¼€ì¤„ ìµœì í™”)ì„ ì‹¤í—˜í•˜ê¸° ì–´ë ¤ì›€ â†’ ì‘ì€ í”„ë¡ì‹œ ëª¨ë¸(proxy model)ì„ í†µí•´ ìµœì ì˜ ë°ì´í„° í˜¼í•© ë¹„ìœ¨, í•™ìŠµ ìŠ¤ì¼€ì¤„ ë“±ì„ ë¨¼ì € íƒìƒ‰í•œ í›„, ì´ë¥¼ ëŒ€ê·œëª¨ ëª¨ë¸ì— ì ìš©
- Training Monitoring : ëŒ€ê·œëª¨ ëª¨ë¸ í›ˆë ¨ì€ **ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë©°**, í›ˆë ¨ ê³¼ì •ì—ì„œ ì†ì‹¤ ìŠ¤íŒŒì´í¬(loss spike)ë‚˜ ë¹„ì •ìƒì  í•™ìŠµ ìƒíƒœê°€ ë°œìƒ ê°€ëŠ¥ â†’ Scaling Lawsë¥¼ í†µí•´ **í›ˆë ¨ ì´ˆê¸° ë‹¨ê³„ì—ì„œ ì´ìƒ ì§•í›„ë¥¼ ê°ì§€**í•˜ê³  ë¬¸ì œë¥¼ ì¡°ê¸°ì— ìˆ˜ì •ê°€

âœ…  Diminishing Returns

- Scaling LawsëŠ” ì¼ë°˜ì ìœ¼ë¡œ **ì„±ëŠ¥ í–¥ìƒì´ ì ì°¨ ë‘”í™”ë˜ëŠ” Diminishing Returns** í˜„ìƒì„ ì˜ˆì¸¡
- ì–¸ì–´ ëª¨ë¸ ì†ì‹¤ì´ ê°ì†Œí•˜ëŠ” í•œ, í‘œí˜„ í’ˆì§ˆ(representation quality)ê³¼ ì˜ë¯¸ì  ì½˜í…ì¸ (semantic content)ëŠ” ê³„ì†í•´ì„œ ê°œì„ ë  ìˆ˜ ìˆìŒ â†’ **ì†ì‹¤ì´ ìˆ˜ë ´ë˜ë”ë¼ë„ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…(Downstream Task)ì—ì„œì˜ ì„±ëŠ¥ì€ ê³„ì† í–¥ìƒë  ê°€ëŠ¥ì„±**ì„ ì˜ë¯¸

âœ…  Data Constraints í•´ê²°

- ë°ì´í„° ë°˜ë³µ ì‚¬ìš© (Data Repetition)
- ë°ì´í„° ì¦ê°• (Data Augmentation)

**â—¼ï¸Task-Level Predictability**

Language Modeling Loss ì¤‘ì‹¬ ì—°êµ¬ì—ì„œ Downstream Tasksì—ì„œì˜ ì„±ëŠ¥ ê°œì„  ì—°êµ¬ë¡œ ë³€í™”

- ì†ì‹¤ ê°ì†ŒëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—… ì„±ëŠ¥ í–¥ìƒê³¼ ì—°ê´€
- ê·¸ëŸ¬ë‚˜ ì¼ë¶€ ì‘ì—…ì—ì„œëŠ” ì†ì‹¤ ê°ì†Œê°€ ì„±ëŠ¥ ì €í•˜ë¥¼ ì´ˆë˜ (Inverse Scaling).
- ì‘ì—… ìˆ˜ì¤€ì˜ Scaling LawsëŠ” **ì‘ì—… ë©”íŠ¸ë¦­, ë‚œì´ë„, ë°ì´í„° í’ˆì§ˆ** ë“± ë‹¤ì–‘í•œ ìš”ì†Œì— ë”°ë¼ ë‹¬ë¼ì§
- **In-Context Learning**ê³¼ ê°™ì€ ì¶œí˜„ì  ëŠ¥ë ¥(Emergent Abilities)ì€ Scaling Lawsë¡œ ì˜ˆì¸¡í•˜ê¸° ì–´ë ¤ì›€

### 3.1.3 Emergent Abilities of LLMs

---

ì‘ì€ ëª¨ë¸ì—ì„œëŠ” ì¡´ì¬í•˜ì§€ ì•Šì§€ë§Œ, ëŒ€ê·œëª¨ ëª¨ë¸ì—ì„œëŠ” ê°‘ì‘ìŠ¤ëŸ½ê²Œ ë“±ì¥í•˜ëŠ” ëŠ¥ë ¥

ëª¨ë¸ ê·œëª¨ê°€ íŠ¹ì • ì„ê³„ê°’(threshold)ì„ ì´ˆê³¼í–ˆì„ ë•Œ **ì„±ëŠ¥ì´ ë¬´ì‘ìœ„(random) ìˆ˜ì¤€ì„ í›¨ì”¬ ì´ˆê³¼**

ë¬¼ë¦¬í•™ì˜ **ìƒì „ì´(Phase Transition)** í˜„ìƒê³¼ ìœ ì‚¬í•œ íŒ¨í„´

**ğŸ§  Emergent Abilities (example)**

---

âœ…  **In-Context Learning (ICL)**

ìì—°ì–´ë¡œ ëœ ì§€ì‹œ(instruction)ë‚˜ ëª‡ ê°€ì§€ ì‘ì—… ì˜ˆì‹œ(demonstrations)ë¥¼ ì œê³µ ë°›ì•˜ì„ ë•Œ, ì¶”ê°€ í•™ìŠµì´ë‚˜ ê²½ì‚¬ í•˜ê°•ë²•(Gradient Update) ì—†ì´ ì£¼ì–´ì§„ ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ë‹¨ì–´ ì‹œí€€ìŠ¤ë¥¼ ì™„ì„±í•˜ì—¬ ê¸°ëŒ€í•˜ëŠ” ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” ëŠ¥ë ¥ â†’ ëª¨ë¸ì´ ì‚¬ì „ í›ˆë ¨ëœ ìƒíƒœë¡œ ì œê³µëœ ì˜ˆì‹œë‚˜ ì§€ì‹œë¥¼ ì´í•´í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ ì‘ì—…ì„ ìˆ˜í–‰

GPT-3 ì´í›„ë¡œ  ICL (ë§¥ë½ ë‚´ í•™ìŠµ ëŠ¥ë ¥ ì¢‹ì•„ì§)

- ICLì€ ë‹¨ìˆœí•œ íŒ¨í„´ ë§¤ì¹­ì´ ì•„ë‹ˆë¼, ëª¨ë¸ì´ ë§¥ë½(Context)ì„ ì´í•´í•˜ê³  ì ì‘í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥
- ëª¨ë¸ ê·œëª¨ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ë” ê°•ë ¥í•˜ê³  ì¼ë°˜í™”ëœ ICL ëŠ¥ë ¥ì´ ë‚˜íƒ€

âœ…  **Instruction Following**

ë‹¤ì–‘í•œ ì‘ì—…ì„ ìì—°ì–´ ì§€ì‹œ(instruction) í˜•íƒœë¡œ ì„¤ëª…í•œ ë©€í‹°íƒœìŠ¤í¬ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •(Fine-tuning)ë˜ì—ˆì„ ë•Œ, ëª…í™•í•œ ì˜ˆì‹œ ì—†ì´ ìƒˆë¡œìš´ ì‘ì—… ì§€ì‹œë¥¼ ë”°ë¥´ëŠ” ëŠ¥ë ¥ â†’ LLMì€ ìì—°ì–´ë¡œ ëœ ì„¤ëª…ì„ í†µí•´ ìƒˆë¡œìš´ ì‘ì—…ì„ ìˆ˜í–‰

Instruction Tuningì„ í†µí•´ ê°•í™”

- ëª¨ë¸ í¬ê¸°ê°€ 68Bì— ë„ë‹¬í–ˆì„ ë•Œ, Instruction Following ëŠ¥ë ¥ì´ í¬ê²Œ í–¥ìƒ
- 8B ì´í•˜ ëª¨ë¸ì—ì„œëŠ” ì´ëŸ° í˜„ìƒì´ ë‚˜íƒ€ë‚˜ì§€ ì•ŠìŒ â†’ ëª¨ë¸ì˜ í¬ê¸°ê°€ **ì„ê³„ê°’ì„ ì´ˆê³¼**í•´ì•¼ ì´ ëŠ¥ë ¥ì´ ëª…í™•í•˜ê²Œ ë‚˜íƒ€ë‚¨

ğŸ“Œ **Instruction Tuning :** ë‹¤ì–‘í•œ ì‘ì—…(Task)ì„ ìì—°ì–´ ì§€ì‹œ(Instruction) í˜•íƒœë¡œ ì„¤ëª…í•œ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ LLMì„ ë¯¸ì„¸ ì¡°ì •(Fine-tuning)í•˜ëŠ” ë°©ë²•

ëª¨ë¸ì´ ì§€ì‹œ(instruction)ë¥¼ ì´í•´í•˜ê³  ë”°ë¥´ëŠ” ëŠ¥ë ¥ì„ í•™ìŠµ â†’ íŠ¹ì • íƒœìŠ¤í¬ì— ê³¼ë„í•˜ê²Œ ìµœì í™”ë˜ì§€ ì•Šê³  **ë²”ìš©ì  ì„±ëŠ¥**ì„ ìœ ì§€

- **ì˜ˆì‹œ:**
    - ì‚¬ìš©ì: *"ë‹¤ìŒ ë¬¸ì¥ì„ ìš”ì•½í•´ì¤˜."*
    - ëª¨ë¸: *"ë¬¼ë¡ ì…ë‹ˆë‹¤. ì—¬ê¸° ìš”ì•½ì…ë‹ˆë‹¤: â€¦"*
    - ì‚¬ìš©ì: *"ë‹¤ìŒ ì½”ë“œë¥¼ ë””ë²„ê¹…í•´ì¤˜."*
    - ëª¨ë¸: *"ì˜¤ë¥˜ëŠ” ì—¬ê¸°ì„œ ë°œìƒí•©ë‹ˆë‹¤: â€¦"*

| Instruction | Input Text | Output Text |
| --- | --- | --- |
| "Translate to French" | "Hello, how are you?" | "Bonjour, comment Ã§a va?" |
| "Summarize the text" | "The quick brown fox..." | "A fox jumped over a dog." |

âœ…  **Step-by-Step Reasoning**

ì—¬ëŸ¬ ë‹¨ê³„ì˜ ë…¼ë¦¬ì  ì¶”ë¡ ì´ í•„ìš”í•œ ë³µì¡í•œ ì‘ì—…(ì˜ˆ: ìˆ˜í•™ ë¬¸ì œ)ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì¤‘ê°„ ì¶”ë¡  ë‹¨ê³„ë¥¼ í†µí•´ ìµœì¢… ë‹µì„ ë„ì¶œí•˜ëŠ” ëŠ¥ë ¥

**Chain-of-Thought (CoT) Prompting** ì „ëµì„ í†µí•´ ê°€ëŠ¥í•´ ì§ 

- ëª¨ë¸ì´ ëª…ì‹œì ì¸ ì¤‘ê°„ ì¶”ë¡  ë‹¨ê³„ë¥¼ í†µí•´ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°
- ì½”ë“œ í•™ìŠµ(Code Training)ì´ ëŠ¥ë ¥ì˜ ì¶œí˜„ì— ì¤‘ìš”í•œ ì—­í• 
- PaLM ë° LaMDA â†’ 60B ì´ìƒì˜ ëª¨ë¸ì—ì„œ CoTê°€ ëª…í™•í•œ ì„±ëŠ¥ í–¥ìƒ
- CoT Promptingì„ í†µí•´ LLMì€ ë³µì¡í•œ ë‹¤ë‹¨ê³„ ë…¼ë¦¬ ì¶”ë¡  ë¬¸ì œë¥¼ í•´ê²°

**ëŠ¥ë ¥ë³„ ì„ê³„ê°’:**

- **ICL:** 13B~175B
- **Instruction Following:** 68B+
- **Step-by-Step Reasoning:** 60B+

### 3.1.4 How Emergent Abilities Relate to Scaling Laws

---

ë‘ ê´€ì ì˜ ì„±ëŠ¥ íŒ¨í„´ ë¹„êµ

| **í•­ëª©** | **Scaling Laws** | **Emergent Abilities** |
| --- | --- | --- |
| **ê°œë…** | ëª¨ë¸ í¬ê¸°, ë°ì´í„° ì–‘, ê³„ì‚°ëŸ‰ì´ ì¦ê°€í•¨ì— ë”°ë¼ ì„±ëŠ¥(ì–¸ì–´ ëª¨ë¸ ì†ì‹¤)ì´ ì–´ë–»ê²Œ ê°œì„ ë˜ëŠ”ì§€ë¥¼ ì„¤ëª…í•˜ëŠ” ê²½í—˜ì  ë²•ì¹™ | ì‘ì€ ëª¨ë¸ì—ì„œëŠ” ë‚˜íƒ€ë‚˜ì§€ ì•Šì§€ë§Œ, ëª¨ë¸ì´ íŠ¹ì • ê·œëª¨ì— ë„ë‹¬í–ˆì„ ë•Œ **ê°‘ì‘ìŠ¤ëŸ½ê²Œ ë“±ì¥í•˜ëŠ” ëŠ¥ë ¥** |
| **ì„±ëŠ¥ íŒ¨í„´** | ì ì§„ì  ê°œì„  (Continuous Improvement) | ê¸‰ê²©í•œ ì„±ëŠ¥ í–¥ìƒ (Sharp Leap) |
| **ì˜ˆì¸¡ ê°€ëŠ¥ì„±** | ë†’ìŒ (Predictable) | ë‚®ìŒ (Unpredictable) |
| **í‰ê°€ ì§€í‘œ** | ì–¸ì–´ ëª¨ë¸ë§ ì†ì‹¤ (Cross-Entropy Loss) | ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—… ì„±ëŠ¥(Task-Specific Performance) |
| **í•œê³„** | ìˆ˜ìµ ì²´ê° (Diminishing Returns) | ì„ê³„ê°’(Threshold)ì´ ë¶ˆëª…í™• |

âœ… **Misaligned Observations (ë‘ ê´€ì  ì°¨ì´)**

**ìŠ¤ì¼€ì¼ë§ ë²•ì¹™**ì€ ì ì§„ì  ê°œì„ (Continuous Improvement)ì„ ì˜ˆì¸¡

**ì¶œí˜„ì  ëŠ¥ë ¥**ì€ ë¶ˆì—°ì†ì  ë„ì•½(Discontinuous Leap)ì„ ë³´ì—¬ì¤Œ

**Evaluation Metrics Difference**

**ìŠ¤ì¼€ì¼ë§ ë²•ì¹™:** ì£¼ë¡œ **Cross-Entropy Loss**ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ í‰ê°€

**ì¶œí˜„ì  ëŠ¥ë ¥:** êµ¬ì²´ì ì¸ ì‘ì—…(Task-Specific Performance)ì˜ ì„±ê³µ ì—¬ë¶€ì— ì¤‘ì  â†’ ì„±ëŠ¥ì´ ì—°ì†ì ì´ê¸°ë³´ë‹¤ ë¶ˆì—°ì†ì ìœ¼ë¡œ ì¸ì‹

ë”°ë¼ì„œ New Evaluation Settingì´ í•„ìš”í•¨ â†’ ì¼ë¶€ ì—°êµ¬ì—ì„œëŠ” ì‘ì—… ì„±ëŠ¥(Task Metrics)ì˜ **í•´ìƒë„(Resolution)**ë¥¼ ë†’ì—¬ ë” ì˜ˆì¸¡ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ì–»ìœ¼ë ¤ê³  í•¨

Fundamental Understanding ì´í•´ë„ ë™ì‹œì— í•„ìš” â†’ â€˜**Grokking**â€™ê³¼ ê°™ì€ í˜„ìƒì„ í†µí•´ LLMì˜ ì‘ë™ ì›ë¦¬ì™€ ì¶œí˜„ì  ëŠ¥ë ¥ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì´í•´í•˜ë ¤ëŠ” ì—°êµ¬ê°€ ì§„í–‰ ì¤‘

**âœ… Analogy with Human Learning**

ë‘ ë²•ì¹™ì˜ ê´€ê³„ëŠ” ì¸ê°„ê³¼ ìœ ì‚¬

- Continuous Improvement : ì–´ë¦°ì´ëŠ” ë§¤ì¼ë§¤ì¼ ì„±ì¥í•˜ì§€ë§Œ, ê·¸ ë³€í™”ëŠ” ëˆˆì— ë„ì§€ ì•ŠìŒ
- Qualitative Leap : ì–´ëŠ ìˆœê°„, ë‹¨ì–´ë¥¼ ë§í•˜ë˜ ì–´ë¦°ì´ê°€ ê°‘ìê¸° **ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë§í•˜ëŠ” ëŠ¥ë ¥**ì„ íšë“
- Step-Wise Changes : ëŠ¥ë ¥ì˜ ì„±ì¥ì€ í•­ìƒ ë§¤ë„ëŸ½ê±°ë‚˜ ì„ í˜•ì ì´ì§€ ì•ŠìŒ

**âœ… Integrated Understanding**

ê´€ì  í†µí•©

- **Scaling Laws â†’ ì ì§„ì  ì„±ì¥ (Continuous Improvement)**
    - ì£¼ì–´ì§„ ê³„ì‚° ì˜ˆì‚°, ë°ì´í„° ì–‘, ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ ì†ì‹¤ì´ ì„œì„œíˆ ì¤„ì–´ë“œëŠ” íŒ¨í„´ì„ ì˜ˆì¸¡
- **Emergent Abilities â†’ ì§ˆì  ë„ì•½ (Qualitative Leap)**
    - ëª¨ë¸ì´ íŠ¹ì • ì„ê³„ê°’ì„ ì´ˆê³¼í–ˆì„ ë•Œ ì˜ˆìƒì¹˜ ëª»í•œ ìƒˆë¡œìš´ ëŠ¥ë ¥ì´ ë‚˜íƒ€ë‚¨
- **Scaling Laws**ì€ **ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì„±ëŠ¥ í–¥ìƒ**ì„ ì œê³µ **Emergent Abilities**ì€ **ë¹„ì•½ì ì¸ ì„±ëŠ¥ ë„ì•½**ì„ ì„¤ëª…

### 3.1.5 Key Techniques for Large Language Models

---

### **âœ… Scaling**

**ëª¨ë¸ í¬ê¸°(Model Size, N), ë°ì´í„° ì–‘(Data Size, D), ê³„ì‚°ëŸ‰(Compute, C)** ì„ ëŠ˜ë¦¬ë©´ ì¼ë°˜ì ìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ

Scaling Lawsì˜ í™œìš© â†’ Compute Budgetì€ í•œì •ì ì´ê¸° ë•Œë¬¸ì—, ìµœì ì˜ ë¦¬ì†ŒìŠ¤ ë°°ë¶„ì´ í•„ìš”

ëª¨ë¸ ì„±ëŠ¥ì€ ë‹¨ìˆœíˆ ë°ì´í„° ì–‘ì´ ì•„ë‹Œ **ë°ì´í„° í’ˆì§ˆ**ì— í¬ê²Œ ì˜ì¡´

### **âœ…** Training

Distributed Training Algorithms í•„ìš” â†’ ëª¨ë¸ í¬ê¸°ê°€ ë„ˆë¬´ ì»¤ì„œ

**Parallel Strategies**

- **Data Parallelism** (ë°ì´í„° ë³‘ë ¬í™”)
- **Model Parallelism** (ëª¨ë¸ ë³‘ë ¬í™”)
- **Pipeline Parallelism** (íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™”)

**Optimization Frameworks**

- **DeepSpeed:** íš¨ìœ¨ì ì¸ ë¶„ì‚° í›ˆë ¨ ì§€ì›.
- **Megatron-LM:** ì´ˆëŒ€í˜• ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•œ ìµœì í™” í”„ë ˆì„ì›Œí¬.

**Stability & Optimization Tricks**

- **Mixed Precision Training:** ê³„ì‚° ì†ë„ í–¥ìƒ ë° ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì í™”.
- **Training Restart:** ì†ì‹¤ ìŠ¤íŒŒì´í¬(loss spike)ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì¬ì‹œì‘ ê¸°ë²•.

**Small Model Prediction**

- ì†Œê·œëª¨ ëª¨ë¸ì„ í†µí•´ ëŒ€ê·œëª¨ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì˜ˆì¸¡

### **âœ… Ability Eliciting**

LLMsëŠ” ëŒ€ê·œëª¨ ë°ì´í„°ë¡œ ì‚¬ì „ í›ˆë ¨ë˜ì§€ë§Œ, ëª¨ë“  ëŠ¥ë ¥ì´ ë°”ë¡œ ë“œëŸ¬ë‚˜ëŠ” ê²ƒì€ ì•„ë‹˜ â†’ LLMsê°€ ìˆ¨ê²¨ì§„ ëŠ¥ë ¥ì„ íš¨ê³¼ì ìœ¼ë¡œ ë°œíœ˜í•˜ë„ë¡ ìœ ë„í•˜ëŠ” ê³¼ì •

**In-Context Learning**

- ì˜ˆì‹œì™€ í•¨ê»˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì œê³µí•˜ì—¬ ëª¨ë¸ì´ ë§¥ë½ì„ ì´í•´í•˜ê³  ë¬¸ì œë¥¼ í•´ê²°

**Chain-of-Thought (CoT) Prompting**

- intermediate reasoning stepsë¥¼ í¬í•¨í•˜ì—¬ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ë„ë¡ ìœ ë„

**Instruction Tuning**

- ë‹¤ì–‘í•œ ì‘ì—…ì„ **ìì—°ì–´ ì§€ì‹œ(Instructions)** í˜•íƒœë¡œ ì„¤ëª…í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì • â†’ ìƒˆë¡œìš´ ì‘ì—…ì—ë„ ë†’ì€ ë²”ìš©ì„±

### **âœ…** Alignment Tuning

Toxic, Biased, Fictitious ì½˜í…ì¸  ìƒì„± ìœ„í—˜  â†’ **Helpful, Honest, Harmless**

**InstructGPT (OpenAI)**

- Reinforcement Learning with Human Feedback, RLHF ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì¸ê°„ì˜ ê¸°ëŒ€ì— ë¶€í•©í•˜ë„ë¡ ì¡°ì •
- **ChatGPT**ëŠ” InstructGPT ê¸°ë°˜ìœ¼ë¡œ ê°œë°œë¨

### **âœ…** Tools Manipulation

LLMsëŠ” **í…ìŠ¤íŠ¸ ê¸°ë°˜ í•™ìŠµ**ì„ í†µí•´ í›ˆë ¨ â†’ **ìˆ˜ì¹˜ ê³„ì‚°, ìµœì‹  ì •ë³´ ì œê³µ** ë“± í…ìŠ¤íŠ¸ë¡œë§Œ í•´ê²°í•˜ê¸° ì–´ë ¤ìš´ ë¬¸ì œ

**Tool Integration ì‚¬ìš©**

- **ê³„ì‚°ê¸° (Calculator):** ì •í™•í•œ ìˆ˜ì¹˜ ê³„ì‚° ì§€ì›
- **ê²€ìƒ‰ ì—”ì§„ (Search Engine):** ì‹¤ì‹œê°„ ìµœì‹  ì •ë³´ ê²€ìƒ‰
- **ì™¸ë¶€ í”ŒëŸ¬ê·¸ì¸ (External Plugins):** ChatGPTëŠ” ì™¸ë¶€ ì•±ì´ë‚˜ í”ŒëŸ¬ê·¸ì¸ì„ í™œìš©í•˜ì—¬ ê¸°ëŠ¥ì„ í™•ì¥

### **âœ… Other Factors**

**í•˜ë“œì›¨ì–´ ì—…ê·¸ë ˆì´ë“œ:** GPU, TPU ë“± í•˜ë“œì›¨ì–´ì˜ ë°œì „

**íš¨ìœ¨ì  ì•Œê³ ë¦¬ì¦˜:** ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ ê°œë°œì´ LLMsì˜ ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„±ì„ ê°œì„ 

<br>

## **3.2** Technical Evolution of GPT-series Models

**Decoder-Only Transformer**

- Next Word Predictionì„ í†µí•´ ì–¸ì–´ ëª¨ë¸ì„ í•™ìŠµ â†’ ëª¨ë¸ì€ ì£¼ì–´ì§„ ë¬¸ë§¥(context)ì—ì„œ ë‹¤ìŒì— ì˜¬ ë‹¨ì–´ë¥¼ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨

**Scaling Up**

- ëª¨ë¸ì˜ í¬ê¸°, ë°ì´í„°ì˜ ì–‘, ê³„ì‚°ëŸ‰ì„ ì§€ì†ì ìœ¼ë¡œ í™•ì¥í•˜ì—¬ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”
- **GPT-3ëŠ” 175B, PaLMì€ 540B íŒŒë¼ë¯¸í„°**ë¥¼ ì‚¬ìš©

**General-Purpose Task Solver**

### **3.2.1** Early Explorations

---

ì´ˆê¸° ì‹¤í—˜ì€ ìˆœí™˜ ì‹ ê²½ë§ (Recurrent Neural Networks, RNNs)ì„ ì‚¬ìš©í•˜ì—¬ ìˆ˜í–‰

ê·¸ëŸ¬ë‚˜ RNNì€ **ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ (Long-Term Dependency Problem)**ë¡œ ì¸í•´ ì„±ëŠ¥ì— í•œê³„ ì¡´ì¬

**2017ë…„ Transformer ì•„í‚¤í…ì²˜ ë“±ì¥** ì´í›„, OpenAIëŠ” Transformerë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì–¸ì–´ ëª¨ë¸ì„ ê°œë°œí•˜ê¸° ì‹œì‘

**âœ… GPT-1 (2018)**

Transformer (Decoder-Only Transformer)

**training technique**

- Unsupervised Pre-training
- Supervised Fine-tuning
- Next Word Prediction

**âœ… GPT-2 (2019)**

- **íŒŒë¼ë¯¸í„° ìˆ˜:** **1.5B**
- **ë°ì´í„°ì…‹:** **WebText** (ëŒ€ê·œëª¨ ì›¹í˜ì´ì§€ ë°ì´í„°ì…‹)
- **ëª©í‘œ:** **ë©€í‹°íƒœìŠ¤í¬ ë¬¸ì œ í•´ê²° (Multi-task Solving)**

**ëª¨ë“  NLP íƒœìŠ¤í¬ëŠ” ë‹¨ì–´ ì˜ˆì¸¡ ë¬¸ì œë¡œ í†µí•©ë  ìˆ˜ ìˆë‹¤."**

**p(output | input, task)**

**Multi-task Solving**

- ì…ë ¥(input), ì¶œë ¥(output), ì‘ì—…(task) ì •ë³´ë¥¼ ìì—°ì–´ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ í†µí•©.
- ë‹¤ì–‘í•œ ì‘ì—…ì´ **ë‹¨ì–´ ì˜ˆì¸¡ ë¬¸ì œë¡œ ë³€í™˜**ë  ìˆ˜ ìˆìŒì„ ì…ì¦.

**Extended principles**

- ì§€ë„ í•™ìŠµ(Supervised Objective)ê³¼ ë¹„ì§€ë„ í•™ìŠµ(Unsupervised Objective)ì€ ë³¸ì§ˆì ìœ¼ë¡œ ë™ì¼í•˜ë©°, í•˜ë‚˜ì˜ ê¸€ë¡œë²Œ ìµœì í™” ëª©í‘œ(Global Minimum)ë¡œ í†µí•©
- ì¶©ë¶„í•œ ëŠ¥ë ¥ì„ ê°€ì§„ ì–¸ì–´ ëª¨ë¸ì€ ë¹„ì§€ë„ í•™ìŠµë§Œìœ¼ë¡œë„ **ë‹¤ì–‘í•œ ì‘ì—…ì„ í•´ê²°í•  ìˆ˜ ìˆë‹¤**ê³  ì£¼ì¥

**Ilya Sutskeverì˜ í†µì°°**

- í…ìŠ¤íŠ¸ ì˜ˆì¸¡ì€ ë‹¨ìˆœí•œ íŒ¨í„´ ë§¤ì¹­ì´ ì•„ë‹ˆë¼, World Knowledgeì˜ ì••ì¶•
- ì–¸ì–´ ëª¨ë¸ì€ í…ìŠ¤íŠ¸ê°€ ìƒì„±ëœ Processì„ í•™ìŠµ, ë‹¨ì–´ ì˜ˆì¸¡ì´ ì •í™•í• ìˆ˜ë¡ ì´ ê³¼ì •ì˜ í•´ìƒë„(Resolution) í–¥ìƒ

ë‹¨ì–´ ì˜ˆì¸¡ì´ ëª¨ë“  NLP ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë‹¨ì¼ ëª©í‘œê°€ ë  ìˆ˜ ìˆìŒ

### **3.2.2** GPT-3 : **Capacity Leap**

---

GPT-2ëŠ” ë¹„ì§€ë„ ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ(Unsupervised Multi-task Learning)ì„ ëª©í‘œë¡œ í–ˆì§€ë§Œ, ì‹¤ì œ ì„±ëŠ¥ì€ ì§€ë„ í•™ìŠµ(Supervised Fine-tuning)ì„ ì‚¬ìš©í•œ ìµœì‹  ëª¨ë¸ë“¤ì— ë¯¸ì¹˜ì§€ ëª»í•¨

GPT-2ëŠ” ë¹„êµì  ì‘ì€ ëª¨ë¸ í¬ê¸°ë¡œ ì¸í•´ ì£¼ë¡œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…(ì˜ˆ: ëŒ€í™” ëª¨ë¸)ì— ë§ì¶¤í˜• ë¯¸ì„¸ ì¡°ì •(Fine-tuning)í•´ ì‚¬ìš©

- **ì¶œì‹œ ì—°ë„:** 2020ë…„
- **íŒŒë¼ë¯¸í„° ìˆ˜:** **175B** (GPT-2ì˜ ì•½ 100ë°°)
- **í•µì‹¬ í˜ì‹ :** **In-Context Learning (ICL)** ë„ì…
- **ëª©í‘œ:** ì§€ë„ í•™ìŠµ ì—†ì´ **Zero-shot** ë˜ëŠ” **Few-shot** ë°©ì‹ìœ¼ë¡œ ë‹¤ì–‘í•œ ì‘ì—…ì„ ìˆ˜í–‰
- **í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„:**
    - **ì‚¬ì „ í•™ìŠµ (Pre-training):** ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ (Next Word Prediction)
    - **In-Context Learning (ICL):** ìì—°ì–´ë¡œ ëœ ì§€ì‹œë‚˜ ì˜ˆì‹œë¥¼ í†µí•´ ìƒˆë¡œìš´ ì‘ì—… ìˆ˜í–‰

> ìì—°ì–´ í…ìŠ¤íŠ¸ë¡œ ì£¼ì–´ì§„ ì§€ì‹œ(Instructions)ì™€ ëª‡ ê°€ì§€ ì˜ˆì‹œ(Demonstrations)ë¥¼ í†µí•´ ì¶”ê°€ í•™ìŠµ ì—†ì´ ìƒˆë¡œìš´ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ëŠ¥ë ¥
> 

- **Zero-shot Learning, Few-shot Learning ëŠ¥ë ¥ í–¥ìƒ**
- **ë‹¤ì–‘í•œ NLP ì‘ì—…ì—ì„œ íƒì›”í•œ ì„±ëŠ¥ ë°œíœ˜:** ì–¸ì–´ ì´í•´, ë²ˆì—­, ì§ˆì˜ì‘ë‹µ ë“±
- **íŠ¹ìˆ˜ ì‘ì—…:** ë³µì¡í•œ ë…¼ë¦¬ ì¶”ë¡ , ë„ë©”ì¸ ì ì‘
- **Emergent Abilitiesì˜ ì¶œí˜„:** ëª¨ë¸ ê·œëª¨ê°€ ì¦ê°€í•˜ë©´ì„œ ìƒˆë¡œìš´ ëŠ¥ë ¥(ì˜ˆ: ë³µì¡í•œ ë…¼ë¦¬ ë¬¸ì œ í•´ê²°)ì´ ë‚˜íƒ€ë‚¨

GPT-3ì˜ ì„±ëŠ¥ì€ ê¸°ì¡´ì˜ **Scaling Laws**ì—ì„œ ì˜ˆì¸¡ëœ ì„±ëŠ¥ ìˆ˜ì¤€ì„ ì´ˆê³¼

ë” í° ëª¨ë¸ì´ ë” ê°•ë ¥í•œ **In-Context Learning (ICL)** ëŠ¥ë ¥ì„ ë³´ì„

â†’ PLMì—ì„œ LLMìœ¼ë¡œì˜ ì „í™˜ì 

### **3.2.2 GPT-3 and GPT-3.5 : Capacity Enhancement**

---

**âœ… Training on Code Data**

- **GPT-3ì˜ í•œê³„:** ë³µì¡í•œ ë…¼ë¦¬ì  ì¶”ë¡ (ì˜ˆ: ì½”ë“œ ìƒì„±, ìˆ˜í•™ ë¬¸ì œ í•´ê²°)ì—ì„œ ë¶€ì¡±í•¨
- **í•´ê²°ì±…:** **Codex (2021)** â€“ GPT-3ë¥¼ ê¸°ë°˜ìœ¼ë¡œ GitHub ì½”ë“œ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•´ ë¯¸ì„¸ ì¡°ì •(Fine-tuning)

í…ìŠ¤íŠ¸ì™€ ì½”ë“œ ì„ë² ë”©(Text and Code Embedding) í•™ìŠµì„ ìœ„í•œ ëŒ€ì¡° í•™ìŠµ(Contrastive Learning) ì ‘ê·¼ë²• ì‚¬ìš©

ì„ í˜• ë¶„ë¥˜(Linear Probe Classification), í…ìŠ¤íŠ¸ ê²€ìƒ‰(Text Search), ì½”ë“œ ê²€ìƒ‰(Code Search)ì— ê°œì„ ëœ ì„±ëŠ¥

**âœ… Human Alignment**

RLHF (Reinforcement Learning with Human Feedback) ê¸°ìˆ  ì‚¬ìš© 

**ìœ í•´í•œ(Toxic)**, **í¸í–¥ëœ(Biased)**, **í—ˆêµ¬ì (Fictitious)** ì½˜í…ì¸  ì—†ì•°

- ì¸ê°„ì´ ì£¼ì„ì„ ë‹¬ì•„ ì„ í˜¸ë„ë¥¼ í‰ê°€

### **3.2.2 T**he Milestones of Language Models

---

**âœ… ChatGPT (2022)**

- **ê¸°ë°˜ ëª¨ë¸:** GPT-3.5, GPT-4
- **í›ˆë ¨ ë°©ì‹:** InstructGPTì™€ ìœ ì‚¬í•œ í›ˆë ¨ í”„ë¡œì„¸ìŠ¤, ì°¨ì´ì  ëŒ€í™”(Conversation)ì— ìµœì í™”ë¨.
    - ì¸ê°„ì´ ìƒì„±í•œ ëŒ€í™” ë°ì´í„°ë¥¼ ì‚¬ìš© (ì‚¬ìš©ìì™€ AI ì—­í• ì„ ëª¨ë‘ í¬í•¨).
    - InstructGPT ë°ì´í„°ì…‹ê³¼ ê²°í•©í•˜ì—¬ í›ˆë ¨.
- Multi-turn Dialogueì—ì„œ Contextë¥¼ ì •í™•í•˜ê²Œ ìœ ì§€ â†’ ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ìˆ˜í–‰
- World Knowledge í¬í•¨

**âœ… GPT-4 (2023)**

- **ì£¼ìš” ë³€í™”:** **í…ìŠ¤íŠ¸ ê¸°ë°˜ ì…ë ¥ â†’ ë©€í‹°ëª¨ë‹¬ ì…ë ¥ (Multimodal Signals)**
- **ê¸°ë°˜ ì•„í‚¤í…ì²˜:** ë””ì½”ë” ì „ìš© Transformer (Decoder-Only Transformer)
- **ëª©í‘œ:** ë³µì¡í•œ ë¬¸ì œ í•´ê²° ë° ì•ˆì „ì„± ê°•í™”

**Multimodal Inputs**

- í…ìŠ¤íŠ¸ë¿ë§Œ ì•„ë‹ˆë¼ ì´ë¯¸ì§€, ê·¸ë˜í”„ ë“± ë‹¤ì–‘í•œ ì…ë ¥ ì‹ í˜¸ë¥¼ ì²˜ë¦¬

ë” ë†’ì€ ìˆ˜ì¤€ì˜ ë…¼ë¦¬ì  ì¶”ë¡  ë° ë³µì¡í•œ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥

**6ê°œì›”ê°„ì˜ ë°˜ë³µì  ì •ë ¬(Iterative Alignment):** RLHF(ì¸ê°„ í”¼ë“œë°±ì„ í†µí•œ ê°•í™” í•™ìŠµ) ì•Œê³ ë¦¬ì¦˜ ê°œì„ .

**ì•ˆì „ ë³´ìƒ ì‹ í˜¸ (Safety Reward Signal):** ìœ í•´í•œ ì‘ë‹µ ë°©ì§€

**Red Teaming:** ì•…ì˜ì  ìš”ì²­ì´ë‚˜ ìœ í•´í•œ ì½˜í…ì¸ ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•œ í…ŒìŠ¤íŠ¸ ê¸°ë²• ë„ì…

**Predictable Scaling**

- ì‘ì€ ê³„ì‚° ìì›ì„ ì‚¬ìš©í•´ ìµœì¢… ì„±ëŠ¥ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ” ê¸°ìˆ  ë„ì…
- í›ˆë ¨ íš¨ìœ¨ì„± ê·¹ëŒ€í™”.

**Optimization mechanisms**

- ì¸í”„ë¼ ê°œì„ ê³¼ ìµœì í™” ê¸°ë²•ìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ ê·¹ëŒ€í™”

í™˜ê°(Hallucination) ë¬¸ì œ ê°ì†Œ

**âœ… GPT-4V ë° GPT-4 Turbo (2023)**

**GPT-4V**

1. **ì‹œê° ëŠ¥ë ¥ (Vision Capabilities):**
    - ì´ë¯¸ì§€ ì…ë ¥ì„ í†µí•´ ë³µì¡í•œ ì‹œê°ì  ë¬¸ì œ í•´ê²° ê°€ëŠ¥
    - ì„¤ëª…, ë¶„ì„, ê°ì²´ ì¸ì‹ ë“± ë‹¤ì–‘í•œ ì‘ì—… ìˆ˜í–‰
2. **ìœ„í—˜ ì™„í™” (Risk Mitigation):**
    - ì‹œê° ì…ë ¥ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ìœ„í—˜ ìš”ì†Œë¥¼ í‰ê°€ ë° ì™„í™”
- AIì˜ **ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥**ì´ ì‹¤ì§ˆì ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ìœ¼ë¡œ í™•ì¥

**GPT-4 Turbo**

- **ëª¨ë¸ ìš©ëŸ‰ í™•ì¥:** GPT-4ë³´ë‹¤ ë” ê°•ë ¥í•œ ì„±ëŠ¥
- **ì§€ì‹ ì†ŒìŠ¤ ì—…ë°ì´íŠ¸:** 2023ë…„ 4ì›”ê¹Œì§€ì˜ ë°ì´í„° í¬í•¨
- **ê¸´ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°:** ìµœëŒ€ **128k í† í°** ì§€ì›
- **ì„±ëŠ¥ ìµœì í™”:** ë¹„ìš© ì ˆê°, ì‘ë‹µ ì†ë„ ê°œì„ 
- **ê¸°ëŠ¥ ì—…ë°ì´íŠ¸:**
    - **Function Call:** ê¸°ëŠ¥ í˜¸ì¶œ ì§€ì›
    - **ì¼ê´€ëœ ì¶œë ¥ (Reproducible Outputs)** ì§€ì›
- **Assistants API:**
    - íŠ¹ì • ëª©í‘œë¥¼ ìˆ˜í–‰í•˜ëŠ” **ì—ì´ì „íŠ¸(Assistants)** ê°œë°œì„ ì‰½ê²Œ ì§€ì›
    - ëª…ë ¹, ë„êµ¬ ì‚¬ìš©, ì¶”ê°€ ì§€ì‹ í†µí•© ê°€ëŠ¥
- **ë©€í‹°ëª¨ë‹¬ í™•ì¥:**
    - **DALLÂ·E 3:** ì´ë¯¸ì§€ ìƒì„±
    - **Text-to-Speech (TTS):** í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜
    - **Voice Samples:** ìŒì„± ìƒ˜í”Œ ì œê³µ

![image.png](/assets/Images/2025-01-08-LLM_survey/image2.png)

TABLE 1: Statistics of large language models (having a size larger than 10B in this survey) in recent years, including the
capacity evaluation, pre-training data scale (either in the number of tokens or storage size) and hardware resource costs.
In this table, we only include LLMs with a public paper about the technical details. Here, â€œRelease Timeâ€ indicates the
date when the corresponding paper was officially released. â€œPublicly Availableâ€ means that the model checkpoints can be
publicly accessible while â€œClosed Sourceâ€ means the opposite. â€œAdaptationâ€ indicates whether the model has been with
subsequent fine-tuning: IT denotes instruction tuning and RLHF denotes reinforcement learning with human feedback.
â€œEvaluationâ€ indicates whether the model has been evaluated with corresponding abilities in their original paper: ICL
denotes in-context learning and CoT denotes chain-of-thought. â€œ*â€ denotes the largest publicly available version.

![image.png](/assets/Images/2025-01-08-LLM_survey/image3.png)