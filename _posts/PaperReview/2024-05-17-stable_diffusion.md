---
title: "[ë…¼ë¬¸ë¶„ì„] High-Resolution Image Synthesis with Latent Diffusion Models"
last_modified_at: 2024-9-25
categories:
  - PaperReview
excerpt: "Multi-model image generation diffusion model"
use_math: true
classes: wide
---

> Image generation with diffusionì— ë°œì „ì— í° ê¸°ì—¬ì„ í•œ ì˜¤í”ˆ ì†ŒìŠ¤ ëª¨ë¸  
[[Page](https://ommer-lab.com/research/latent-diffusion-models/), [Paper](https://arxiv.org/abs/2112.10752), [Code](https://github.com/CompVis/stable-diffusion)]  
LMU Munich, IWR, Heidelberg University, Runway
CVPR 2022 (ORAL)

<br>

# **Abstract**

By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state-of-the-art scores for image impainting and class-conditional image synthesis and highly competitive performance on various tasks, including text-to-image synthesis, unconditional image generation and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.

diffusion model ì€ ì´ë¯¸ì§€ ìƒì„±ì—ì„œ ë›°ì–´ë‚¨

í”½ì…€ ê³µê°„ì—ì„œ ë™ì‘í•˜ëŠ” diffusion modelì€ ìµœì í™”í•˜ëŠ” ë° ë¹„ìš©ì´ ë§ì´ ë“¬

latent spaceì—ì„œ ë™ì‘í•˜ì—¬, í’ˆì§ˆê³¼ ìœ ì—°ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ì œí•œëœ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ í™œìš©

ì˜¤í† ì¸ì½”ë”ì˜ ì ì¬ ê³µê°„ì— ì ìš©

ë³µì¡ì„± ê°ì†Œì™€ ë””í…Œì¼ ë³´ì¡´ ì‚¬ì´ì˜ ìµœì ì ì— ë„ë‹¬

ëª¨ë¸ ì•„í‚¤í…ì²˜ì— cross attention layerë¥¼ í†µí•´ condition ì…ë ¥

ë©€í‹°ëª¨ë‹¬ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸

<br>

# Introduction

ì´ ê°•ë ¥í•œ ëª¨ë¸ í´ë˜ìŠ¤ì˜ ì ‘ê·¼ì„±ì„ ë†’ì´ëŠ” ë™ì‹œì— ìƒë‹¹í•œ ë¦¬ì†ŒìŠ¤ ì†Œë¹„ë¥¼ ì¤„ì´ë ¤ë©´ í›ˆë ¨ê³¼ ìƒ˜í”Œë§ ëª¨ë‘ì— ëŒ€í•œ ê³„ì‚° ë³µì¡ì„±ì„ ì¤„ì´ëŠ” ë°©ë²•ì´ í•„ìš” â†’ Latent Spaceë¥¼ í™œìš©

í•™ìŠµëœ ëª¨ë¸ì˜ ì†ë„-ì™œê³¡ íŠ¸ë ˆì´ë“œ ì˜¤í”„

![Figure 2. Illustrating perceptual and semantic compression: Most bits of a digital image correspond to imperceptible details. While DMs allow to suppress this semantically meaningless information by minimizing the responsible loss term, gradients (during training) and the neural network backbone (training and inference) still need to be evaluated on all pixels, leading to superfluous computations and unnecessarily expensive optimization and inference. We propose latent diffusion models (LDMs) as an effective generative model and a separate mild compression stage that only eliminates imperceptible details. Data and images from [30].](/assets/Images/2024-05-17-stable_diffusion/Untitled.png)

Figure 2. Illustrating perceptual and semantic compression: Most bits of a digital image correspond to imperceptible details. While DMs allow to suppress this semantically meaningless information by minimizing the responsible loss term, gradients (during training) and the neural network backbone (training and inference) still need to be evaluated on all pixels, leading to superfluous computations and unnecessarily expensive optimization and inference. We propose latent diffusion models (LDMs) as an effective generative model and a separate mild compression stage that only eliminates imperceptible details. Data and images from [30].

1. ì§€ê°ì  ì••ì¶• ë‹¨ê³„(Perception Compression)ë¡œ ë¹ˆë„ê°€ ë†’ì€ ì„¸ë¶€ ì •ë³´ë¥¼ ì œê±°í•˜ì§€ë§Œ ì˜ë¯¸ì  ë³€í™”ëŠ” ê±°ì˜ í•™ìŠµí•˜ì§€ ì•ŠìŒ
2. ì‹¤ì œ ìƒì„± ëª¨ë¸ì´ ë°ì´í„°ì˜ ì˜ë¯¸ì  ë° ê°œë…ì  êµ¬ì„±ì„ í•™ìŠµ(Semantic Compression). ì§€ê°ì ìœ¼ë¡œëŠ” ë™ë“±í•˜ì§€ë§Œ ê³„ì‚°ì ìœ¼ë¡œëŠ” ë” ì í•©í•œ ê³µê°„ì„ ì°¾ì•„ ê³ í•´ìƒë„ ì´ë¯¸ì§€ í•©ì„±ì„ ìœ„í•œ ì°¨ì´ ëª¨ë¸ì„ í›ˆë ¨

í›ˆë ¨ì„ ë‘ ë‹¨ê³„ë¡œ ë¶„ë¦¬

1. ë°ì´í„° ê³µê°„ê³¼ ì§€ê°ì ìœ¼ë¡œ ë™ë“±í•œ ì €ì°¨ì›(ë”°ë¼ì„œ íš¨ìœ¨ì ì¸) í‘œí˜„ ê³µê°„ì„ ì œê³µí•˜ëŠ” ìë™ ì¸ì½”ë”ë¥¼ í›ˆë ¨
2. ê³µê°„ ì°¨ì›ê³¼ ê´€ë ¨í•˜ì—¬ ë” ë‚˜ì€ í™•ì¥ íŠ¹ì„±ì„ ë³´ì´ëŠ” í•™ìŠµëœ ì ì¬ ê³µê°„ì—ì„œ DMì„ í›ˆë ¨, ì§€ì†ì ì¸ ê³µê°„ ì••ì¶•ì— ì˜ì¡´í•  í•„ìš”ê°€ ì—†ìŒ. ë³µì¡ì„±ì´ ê°ì†Œí•˜ì—¬ ë‹¨ì¼ ë„¤íŠ¸ì›Œí¬ íŒ¨ìŠ¤ë¡œ ì ì¬ ê³µê°„ì—ì„œ íš¨ìœ¨ì ì¸ ì´ë¯¸ì§€ ìƒì„±ì´ ê°€ëŠ¥. (LDM)

ë²”ìš© ìë™ ì¸ì½”ë”© ë‹¨ê³„ë¥¼ í•œ ë²ˆë§Œ í›ˆë ¨ â†’ ì—¬ëŸ¬ DM í›ˆë ¨ì— ì¬ì‚¬ìš©í•˜ê±°ë‚˜ ì™„ì „íˆ ë‹¤ë¥¸ ì‘ì—…ì„ íƒìƒ‰í•  ìˆ˜ ìˆìŒ

íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ DMì˜ UNet ë°±ë³¸ì— ì—°ê²°, ìœ í˜•ì˜ í† í° ê¸°ë°˜ ì»¨ë””ì…”ë‹ ë©”ì»¤ë‹ˆì¦˜ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì•„í‚¤í…ì²˜ë¥¼ ì„¤ê³„

<br>

# Method

diffusion modelì— trainingì—ì„œ high qualityë¥¼ ìœ„í•œ computational costë¥¼ ë‚®ì¶”ê¸° ìœ„í•´ undersampling ì‚¬ìš© â†’ í•˜ì§€ë§Œ, í”½ì…€ ê³µê°„ì—ì„œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •ì€ ì—¬ì „íˆ ë§ì€ ê³„ì‚° ìì›ì„ í•„ìš”

undersampling : ì „ì²´ ë°ì´í„° ì¤‘ì—ì„œ ì¼ë¶€ë§Œ ì„ íƒí•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ë°©ë²•,  ì£¼ë¡œ ë°ì´í„°ì˜ íŠ¹ì • ë¶€ë¶„ì„ ë¬´ì‹œí•˜ê±°ë‚˜ ëœ ì¤‘ìš”í•˜ê²Œ ì²˜ë¦¬í•¨ìœ¼ë¡œì¨ ì²˜ë¦¬ ì†ë„ë¥¼ ë†’ì´ê³  ê³„ì‚° ìì›ì„ ì ˆì•½í•˜ëŠ” ë° ì‚¬ìš©

ë”°ë¼ì„œ auto-encoder ì‚¬ìš© 

---

<br>

## **Perceptual Image Compression (AutoEncoder)**

<aside>
ğŸ’¡ ì˜¤í† ì¸ì½”ë”ì˜ í•™ìŠµì€ ì§€ê°ì  ì†ì‹¤ í•¨ìˆ˜(perceptual loss) ì‚¬ìš© : ì§€ê°ì  ì†ì‹¤ì€ ì´ë¯¸ì§€ì˜ ì‹œê°ì  í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” í•¨ìˆ˜ë¡œ, ì´ë¯¸ì§€ì˜ ê³ ìˆ˜ì¤€ íŠ¹ì§•ì„ ìœ ì§€

íŒ¨ì¹˜ ê¸°ë°˜ ì ëŒ€ì  ëª©í‘œ(Patch-based Adversarial Objective) : ì´ë¯¸ì§€ì˜ ì¼ë¶€ë¶„(íŒ¨ì¹˜)ì— ëŒ€í•´ ì ëŒ€ì  ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ì§€ì—­ì (realism) í˜„ì‹¤ê°ì„ ë³´ì¥í•˜ë„ë¡ í•™ìŠµ

ì§€ê°ì  ì†ì‹¤ê³¼ íŒ¨ì¹˜ ê¸°ë°˜ ì ëŒ€ì  ëª©í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ ë¸”ëŸ¬ íš¨ê³¼ë¥¼ ì¤„ì´ê³  ì´ë¯¸ì§€ì˜ ì„ ëª…ë„ë¥¼ ë†’ì„
(L2 ë˜ëŠ” L1 ëª©í‘œì™€ ê°™ì€ í”½ì…€ ê³µê°„ ì†ì‹¤ì—ë§Œ ì˜ì¡´í•˜ë©´ íë¦¿í•¨)

</aside>

<br>

### Perceptual Loss

$$
\mathcal{L}_{\text{perceptual}} = \frac{1}{CHW} \sum_{c=1}^{C} \sum_{h=1}^{H} \sum_{w=1}^{W} \left( \phi(x)_{c,h,w} - \phi(\hat{x})_{c,h,w} \right)^2
$$

$$\mathcal{L}_{perceptual}$$ : ì§€ê°ì  ì†ì‹¤ í•¨ìˆ˜

$$Ï•(x)$ì™€ $Ï•(\hat{x})$$ëŠ” ì‚¬ì „ í›ˆë ¨ëœ ì‹ ê²½ë§ì—ì„œ ì¶”ì¶œí•œ ì›ë³¸ ì´ë¯¸ì§€

$$x$$ ì™€ ìƒì„±ëœ ì´ë¯¸ì§€ $$\hat{x}$$ ì˜ íŠ¹ì§• ë§µ

CëŠ” íŠ¹ì§• ë§µì˜ ì±„ë„ ìˆ˜, HëŠ” ë†’ì´, WëŠ” ë„ˆë¹„

$$Ï•(x)_{c,h,w}$$ ì™€  $$Ï•(\hat{x})_{c,h,w}$$ ëŠ” ì±„ë„ c, ë†’ì´ h, ë„ˆë¹„ wìœ„ì¹˜ì—ì„œì˜ íŠ¹ì§• ë§µ ê°’

<br>

### Encoder and Decoder

$$x âˆˆ R^{HÃ—WÃ—3}$$   â†’   $$z = Îµ(x)$$  ,   $$z âˆˆ R^{hÃ—wÃ—c}$$   â†’          $$x = D(z) = D(Îµ(x))$$

  Îµ : encoder

  D : decoder

$$f = H/h = W/w$$  :  the encoder *downsamples* the image by a factor

$$f = 2^m$$ ,  with $$m âˆˆ \mathbb{N}$$ :  downsampling factors â†’ ì´ë¯¸ì§€ì˜ í•´ìƒë„ë¥¼ ì¡°ì •í•˜ëŠ” ì¤‘ìš”í•œ ê²°ì • ìš”ì†Œ

- ë‹¤ë¥¸ sampling factorë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ 
    1. ë™ì¼í•œ ìƒ˜í”Œë§ íŒ©í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, ëª¨ë¸ì€ íŠ¹ì • í•´ìƒë„ì—ì„œë§Œ ìµœì í™”. 
    2. ë‹¤ì–‘í•œ ìƒ˜í”Œë§ íŒ©í„°ë¥¼ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì´ ë‹¤ì–‘í•œ í•´ìƒë„ì—ì„œ í•™ìŠµí•  ìˆ˜ ìˆê²Œ ë˜ì–´, ì‹¤ì œ ìƒí™©ì—ì„œ ë‹¤ì–‘í•œ ì¡°ê±´ì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° ìœ ë¦¬
    3. VAEì™€ ê°™ì€ ëª¨ë¸ì—ì„œ ì ì¬ ê³µê°„(latent space)ì˜ í¬ê¸°ì™€ êµ¬ì¡°ëŠ” ë°ì´í„°ì˜ ë³µì¡ì„±ê³¼ ë‹¤ì–‘ì„±ì„ ë°˜ì˜. ìƒ˜í”Œë§ íŒ©í„°ê°€ ë™ì¼í•˜ë©´ ì ì¬ ê³µê°„ì˜ í‘œí˜„ì´ ì œí•œ. 
    
    ë‹¤ì–‘í•œ ìƒ˜í”Œë§ íŒ©í„°ë¥¼ ì‚¬ìš© â†’ ì ì¬ ê³µê°„ì´ ë” í’ë¶€í•˜ê³  ìœ ì˜ë¯¸í•œ í‘œí˜„ì„ ê°€ì§, ë³µì¡í•œ ë°ì´í„° êµ¬ì¡°ë¥¼ ë” ì˜ í•™ìŠµ
    

<br>

### latent spaceì˜ ê³ ë¶„ì‚°ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‚¬ìš©ëœ ë‘ ê°€ì§€ ì •ê·œí™” ê¸°ë²•

1. KL ì •ê·œí™”(KL-reg.) : Variational Autoencoder (VAE)ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ê³¼ ìœ ì‚¬
    
    í•™ìŠµëœ ì ì¬ ê³µê°„ì„ í‘œì¤€ ì •ê·œ ë¶„í¬ì— ê°€ê¹Œì›Œì§€ë„ë¡ í•˜ëŠ” KL-íŒ¨ë„í‹°(Kullback-Leibler Divergence)ë¥¼ ë¶€ê³¼
    
2. ë²¡í„° ì–‘ìí™” ì •ê·œí™”(VQ-reg.) : ë²¡í„° ì–‘ìí™”(Vector Quantization) ë ˆì´ì–´ë¥¼ ë””ì½”ë”ì— í¬í•¨ì‹œì¼œ ì‚¬ìš©
    
    VQGAN(Vector Quantized Generative Adversarial Network)ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ, ì–‘ìí™” ë ˆì´ì–´ê°€ ë””ì½”ë”ì— í¡ìˆ˜ë˜ì–´ ìˆë‹¤ëŠ” ì°¨ì´
    
    ì–‘ìí™” ë ˆì´ì–´ëŠ” í•™ìŠµëœ ì ì¬ ê³µê°„ì„ ì œí•œëœ ìˆ˜ì˜ ê³ ì •ëœ ë²¡í„°ë¡œ í‘œí˜„í•˜ì—¬ ê³ ë¶„ì‚°ì„ ì–µì œ
    

ì—¬ê¸°ì„œëŠ” VQGANì„ ì‚¬ìš©

![Untitled](/assets/Images/2024-05-17-stable_diffusion/Untitled%201.png)

ì´ ëª¨ë¸ì€ í•™ìŠµëœ ì ì¬ ê³µê°„ $z=E(x)$ ì˜ 2ì°¨ì› êµ¬ì¡°ë¥¼ í™œìš©í•˜ì—¬ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ ì••ì¶•ë¥ ì„ ì‚¬ìš©í•˜ë©´ì„œë„ ë§¤ìš° ìš°ìˆ˜í•œ ë³µì› ì„±ëŠ¥ì„ ë‹¬ì„± â†’ ì´ì „ ì—°êµ¬ë“¤ì€ í•™ìŠµëœ ì ì¬ ê³µê°„ì„ ì„ì˜ì˜ 1ì°¨ì› ìˆœì„œë¡œ ë°°ì—´í•˜ì—¬ ë¶„í¬ë¥¼ ëª¨ë¸ë§í–ˆê¸° ë•Œë¬¸ì—, ì ì¬ ê³µê°„ì˜ ë‚´ì¬ëœ êµ¬ì¡°ë¥¼ ë¬´ì‹œ

â†’ ì›ë³¸ ì´ë¯¸ì§€ì˜ ì¤‘ìš”í•œ íŠ¹ì§•ì„ ë” ì˜ ë³µì› (ì•„ë§ˆ ì´ë¯¸ì§€ì´ê¸° ë•Œë¬¸ì— 2ì°¨ì›?)

---

<br>

## **Latent Diffusion Models**

- **Diffusion Models** (DDPM)
    
    $$
    L_{DM} = \mathbb{E}_{x, \epsilon \sim \mathcal{N}(0,1), t} \left[ \left\| \epsilon - \epsilon_\theta (x_t, t) \right\|_2^2 \right]
    $$
    
    with t uniformly sampled from {1, . . . , T}
    
- **Generative Modeling of Latent Representations**
    
    $$
    L_{LDM} := \mathbb{E}_{\mathcal{E}(x), \epsilon \sim \mathcal{N}(0,1), t} \left[ \left\| \epsilon - \epsilon_\theta (z_t, t) \right\|_2^2 \right].
    $$
    
    $$Îµ_Î¸(â—¦, t)$$ ì€ UNetìœ¼ë¡œ êµ¬í˜„
    

![Untitled](/assets/Images/2024-05-17-stable_diffusion/Untitled%202.png)

---

<br>

## **Conditioning Mechanisms**

ì›ì¹™ì ìœ¼ë¡œ $p(z|y)$ í˜•ì‹ì˜ ì¡°ê±´ë¶€ ë¶„í¬ë¥¼ ëª¨ë¸ë§ â†’ ì¡°ê±´ë¶€ ë…¸ì´ì¦ˆ ì œê±° ìë™ ì¸ì½”ë” $Îµ_Î¸(z_t, t, y)$ë¡œ êµ¬í˜„

ë‹¤ì–‘í•œ ì…ë ¥ ëª¨ë‹¬ë¦¬í‹°ì˜ Attention based ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë° íš¨ê³¼ì ì¸ êµì°¨ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ê¸°ë³¸ UNet ë°±ë³¸ì„ ë³´ê°•í•˜ì—¬ Diffusion Modelì„ ë³´ë‹¤ ìœ ì—°í•œ conditional image generatorë¡œ ì „í™˜

domain specific encoder $$Ï„_Î¸$   :   $Ï„_Î¸(y) âˆˆ R^{MÃ—d_Ï„}$$

<br>

### UNetì— cross-attention

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right) \cdot V
$$

$$
Q = W_Q^{(i)} \cdot \varphi_i(z_t), \quad K = W_K^{(i)} \cdot \tau_\theta(y), \quad V = W_V^{(i)} \cdot \tau_\theta(y). \\
$$

$$\varphi_i(z_t) \in \mathbb{R}^{N \times d_{\epsilon}^i}$$  :  Ï†iëŠ” ië²ˆì§¸ ì¸µ ë˜ëŠ” ëª¨ë“ˆ(UNet)ì—ì„œ ìƒì„±ëœ ì¤‘ê°„ íŠ¹ì§• ë²¡í„°

$$\epsilon_\theta \text{ and } W_V^{(i)} \in  \mathbb{R}^{d \times d_\epsilon^i}, \quad W_Q^{(i)} \in \mathbb{R}^{d \times d_\tau} \quad \& \quad W_K^{(i)} \in \mathbb{R}^{d \times d_\tau} \quad$$:  learnable projection matrices

$$\epsilon_\theta$$  :  ëª¨ë¸ì—ì„œ ì˜ˆì¸¡í•œ ì¡ìŒ(noise)ì„ ë‚˜íƒ€ë‚´ëŠ” ë³€ìˆ˜,  Î¸ëŠ” ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜

- Query : í˜„ì¬ì˜ ì…ë ¥ì´ ë‹¤ë¥¸ ì…ë ¥ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ì§€ ì¸¡ì •í•˜ëŠ” ë° ì‚¬ìš© , $$\varphi_i(z_t)$$ ì„ ì‚¬ìš©
- Key  : Queryì™€ì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ëŠ” ê¸°ì¤€ , $$Ï„_Î¸$$  ì„ ì‚¬ìš©
- Value : ìµœì¢… ì¶œë ¥ì— í¬í•¨ë  ì •ë³´ , 
$$Ï„_Î¸$$
 ì„ ì‚¬ìš©

cross attentionì„ í†µí•´ domain specific encoderì˜ ì¶œë ¥ì„ ë‚´ë³´ëƒ„

### ìµœì¢… ìˆ˜ì‹

$$
L_{LDM} := \mathbb{E}_{\mathcal{E}(x), y, \epsilon \sim \mathcal{N}(0,1), t} \left[ \left\| \epsilon - \epsilon_\theta (z_t, t, \tau_\theta (y)) \right\|_2^2 \right]
$$

<br>

# **Experiments**

![Untitled](/assets/Images/2024-05-17-stable_diffusion/Untitled%203.png)

<br>

## **On Perceptual Compression Tradeoffs**

different downsampling factors f âˆˆ {1, 2, 4, 8, 16, 32}ì„  í™•ì¸

LDM-f (latent space) , LDM-1 (pixel space)

![Figure 6. Analyzing the training of class-conditional LDMs with different downsampling factors f over 2M train steps on the Im- ageNet dataset. Pixel-based LDM-1 requires substantially larger train times compared to models with larger downsampling factors (LDM-{4-16}). Too much perceptual compression as in LDM-32 limits the overall sample quality. All models are trained on a single NVIDIA A100 with the same computational budget. Results obtained with 100 DDIM steps [84] and Îº = 0.](/assets/Images/2024-05-17-stable_diffusion/Untitled%204.png)

Figure 6. Analyzing the training of class-conditional LDMs with different downsampling factors f over 2M train steps on the Im- ageNet dataset. Pixel-based LDM-1 requires substantially larger train times compared to models with larger downsampling factors (LDM-{4-16}). Too much perceptual compression as in LDM-32 limits the overall sample quality. All models are trained on a single NVIDIA A100 with the same computational budget. Results obtained with 100 DDIM steps [84] and Îº = 0.

LDM-{1,2}ì˜ ë‹¤ìš´ìƒ˜í”Œë§ ì¸ìê°€ ì‘ìœ¼ë©´ í›ˆë ¨ ì§„í–‰ ì†ë„ê°€ ëŠë ¤ì§€ëŠ” ë°˜ë©´, ii) f ê°’ì´ ì§€ë‚˜ì¹˜ê²Œ í¬ë©´ ë¹„êµì  ì ì€ í›ˆë ¨ ë‹¨ê³„ í›„ì— ì¶©ì‹¤ë„ê°€ ì €í•˜ë˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤

![Untitled](/assets/Images/2024-05-17-stable_diffusion/Untitled%205.png)

![Untitled](/assets/Images/2024-05-17-stable_diffusion/Untitled%206.png)

ìœ„ì˜ ë¶„ì„(ê·¸ë¦¼ 1ê³¼ 2)ì„ ë‹¤ì‹œ ì‚´í´ë³´ë©´, ì´ëŠ” i) Semantic Compressionì˜ ëŒ€ë¶€ë¶„ì„ í™•ì‚° ëª¨ë¸ì— ë§¡ê¸°ê³ , ii) ë„ˆë¬´ ê°•í•œ ì²« ë‹¨ê³„ ì••ì¶•ìœ¼ë¡œ ì¸í•´ ì •ë³´ê°€ ì†ì‹¤ë˜ì–´ í’ˆì§ˆ ì €í•˜ ë°œìƒ

LDM-{4-16}ì€ íš¨ìœ¨ì„±ê³¼ ì§€ê°ì ìœ¼ë¡œ ì¶©ì‹¤í•œ ê²°ê³¼ ì‚¬ì´ì˜ ê· í˜•ì„ ì˜ ë§ì¶”ê³  ìˆìœ¼ë©°, ì´ëŠ” 2M í›ˆë ¨ ë‹¨ê³„ ì´í›„ í”½ì…€ ê¸°ë°˜ í™•ì‚°(LDM-1)ê³¼ LDM-8 ì‚¬ì´ì˜ ìœ ì˜ë¯¸í•œ FID ê²©ì°¨ 38ë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.

![Figure 7. Comparing LDMs
CelebA-HQ (left) and ImageNet (right) datasets. Different markers indicate {10, 20, 50, 100, 200} sampling steps using DDIM, from right to left along each line. The dashed line shows the FID scores for 200 steps, indicating the strong performance of LDM- {4-8}. FID scores assessed on 5000 samples. All models were trained for 500k (CelebA) / 2M (ImageNet) steps on an A100.](/assets/Images/2024-05-17-stable_diffusion/Untitled%207.png)

Figure 7. Comparing LDMs
CelebA-HQ (left) and ImageNet (right) datasets. Different markers indicate {10, 20, 50, 100, 200} sampling steps using DDIM, from right to left along each line. The dashed line shows the FID scores for 200 steps, indicating the strong performance of LDM- {4-8}. FID scores assessed on 5000 samples. All models were trained for 500k (CelebA) / 2M (ImageNet) steps on an A100.

í”½ì…€ ê¸°ë°˜ì— ë¹„í•´ í™•ì‹¤íˆ ë¹ ë¥¸ ì²˜ë¦¬ì†ë„

ì´ë¯¸ì§€ë„·ê³¼ ê°™ì€ ë³µì¡í•œ ë°ì´í„° ì„¸íŠ¸ëŠ” í’ˆì§ˆ ì €í•˜ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì••ì¶•ë¥ ì„ ë‚®ì¶”ëŠ”ê²Œ ì¢‹ìŒ

LDM-4ì™€ -8ì´ ë² ìŠ¤íŠ¸

<br>

## **Image Generation with Latent Diffusion**

Uncondition result

![Table 1. Evaluation metrics for unconditional image synthesis. CelebA-HQ results reproduced from [43, 63, 100], FFHQ from [42, 43]. â€  : N -s refers to N sampling steps with the DDIM [84] sampler. âˆ—: trained in KL-regularized latent space. Additional re- sults can be found in the supplementary.](/assets/Images/2024-05-17-stable_diffusion/Untitled%208.png)

Table 1. Evaluation metrics for unconditional image synthesis. CelebA-HQ results reproduced from [43, 63, 100], FFHQ from [42, 43]. â€  : N -s refers to N sampling steps with the DDIM [84] sampler. âˆ—: trained in KL-regularized latent space. Additional results can be found in the supplementary.

text condition

![Table 2. Evaluation of text-conditional image synthesis on the 256 Ã— 256-sized MS-COCO [51] dataset: with 250 DDIM [84] steps our model is on par with the most recent diffusion [59] and autoregressive [26] methods despite using significantly less pa- rameters. â€ /âˆ—:Numbers from [109]/ [26]](/assets/Images/2024-05-17-stable_diffusion/Untitled%209.png)

Table 2. Evaluation of text-conditional image synthesis on the 256 Ã— 256-sized MS-COCO [51] dataset: with 250 DDIM [84] steps our model is on par with the most recent diffusion [59] and autoregressive [26] methods despite using significantly less pa- rameters. â€ /âˆ—:Numbers from [109]/ [26]

latent spaceì—ì„œëŠ” ë§¤ê°œ ë³€ìˆ˜ì˜ ì ˆë°˜ì„ ì‚¬ìš©í•˜ê³  4ë°° ì ì€ í›ˆë ¨ ë¦¬ì†ŒìŠ¤ë¥¼ ë‹¤ì‹œ ì‚¬ìš©í•¨ ê·¸ëŸ¼ì—ë„ ì„±ëŠ¥ì´ ë” ì¢‹ìŒ

![Figure 4. Samples from *LDMs* trained on CelebAHQ [39], FFHQ [41], LSUN-Churches [102], LSUN-Bedrooms [102] and class-conditional ImageNet [12], each with a resolution of 256 Ã— 256. Best viewed when zoomed in. For more samples *cf* . the supplement](/assets/Images/2024-05-17-stable_diffusion/Untitled%2010.png)

Figure 4. Samples from *LDMs* trained on CelebAHQ [39], FFHQ [41], LSUN-Churches [102], LSUN-Bedrooms [102] and class-conditional ImageNet [12], each with a resolution of 256 Ã— 256. Best viewed when zoomed in. For more samples *cf* . the supplement.

	

<br>

## **Conditional Latent Diffusion**

### **Transformer Encoders for LDMs**

LDMì— cross attentionì„ ë„ì…í•´ ë‹¤ì–‘í•œ conditionì„ ë°›ì„ ìˆ˜ ìˆë„ë¡ í•¨

BERT-í† í°ë¼ì´ì €ë¥¼ ì‚¬ìš©í•˜ê³  Ï„Î¸ë¥¼ ë³€í™˜ê¸°ë¡œ êµ¬í˜„í•˜ì—¬ (multi head) cross attentionë¥¼ í†µí•´ UNetì— ë§¤í•‘ë˜ëŠ” ì ì¬ ì½”ë“œë¥¼ ì¶”ë¡ 

ì–¸ì–´ í‘œí˜„ í•™ìŠµì„ ìœ„í•œ BERT-í† í°ë¼ì´ì €ì™€ ì‹œê°ì  í•©ì„±ì„ ìœ„í•œ diffusionì˜ ì¡°í•©

![Untitled](/assets/Images/2024-05-17-stable_diffusion/Untitled%2011.png)

![Untitled](/assets/Images/2024-05-17-stable_diffusion/Untitled%2012.png)

classifier-free diffusion guidanceë¥¼ ì ìš©í•˜ë©´ ìƒ˜í”Œì˜ í’ˆì§ˆì´ í¬ê²Œ í–¥ìƒ (*LDM-KL-8-G*) ë™ì‹œì— íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ í¬ê²Œ ì¤„ì„

### **Convolutional Sampling Beyond**  $256^2$

ê³µê°„ì ìœ¼ë¡œ ì •ë ¬ëœ ì»¨ë””ì…”ë‹ ì •ë³´ë¥¼ ÎµÎ¸ì˜ ì…ë ¥ì— ì—°ê²° â†’ LDMì€ íš¨ìœ¨ì ì¸ ë²”ìš© ì´ë¯¸ì§€ ê°„ ë²ˆì—­ ëª¨ë¸ë¡œ ì‚¬ìš©

ì˜ë¯¸ë¡ ì  í•©ì„± : í’ê²½ ì´ë¯¸ì§€ì™€ ì˜ë¯¸ë¡ ì  ì§€ë„(semantic maps)ë¥¼ ì‚¬ìš©

![Untitled](/assets/Images/2024-05-17-stable_diffusion/Untitled%2013.png)

256x256 í•´ìƒë„ì—ì„œ í›ˆë ¨í•˜ì§€ë§Œ, ëª¨ë¸ì€ ë” í° í•´ìƒë„ì—ì„œë„ ì¼ë°˜í™”ë  ìˆ˜ ìˆìœ¼ë©°, ë©”ê°€í”½ì…€ í¬ê¸°ì˜ ì´ë¯¸ì§€ë„ ìƒì„± ê°€ëŠ¥

<br>

## **Super-Resolution with Latent Diffusion**

![Figure 10. ImageNet 64â†’256 super-resolution on ImageNet-Val. LDM-SR has advantages at rendering realistic textures but SR3 can synthesize more coherent fine structures. See appendix for additional samples and cropouts. SR3 results from [72].](/assets/Images/2024-05-17-stable_diffusion/Untitled%2014.png)

Figure 10. ImageNet 64â†’256 super-resolution on ImageNet-Val. LDM-SR has advantages at rendering realistic textures but SR3 can synthesize more coherent fine structures. See appendix for additional samples and cropouts. SR3 results from [72].

â€˜concatenationâ€™ì„ í†µí•´ ì €í•´ìƒë„ ì´ë¯¸ì§€ì— ì§ì ‘ ì»¨ë””ì…”ë‹í•¨ìœ¼ë¡œì¨ LDMì„ íš¨ìœ¨ì ìœ¼ë¡œ ì´ˆí•´ìƒë„ë¡œ í›ˆë ¨

ì¼ë¶€ëŸ¬ ì €í•´ìƒë„ë¡œ ë‚®ì¶°ì„œ resolutionì„ ì§„í–‰

- **ì‹¤í—˜**:
    - SR3[72] ëª¨ë¸ì„ ë”°ë¼ 4ë°° ë‹¤ìš´ìƒ˜í”Œë§ì„ í†µí•´ ì´ë¯¸ì§€ë¥¼ low-resolutionì‹œí‚¤ê³ , ì´ë¥¼ interpolationìœ¼ë¡œ ìˆ˜ì •í•©ë‹ˆë‹¤.
    - SR3ì˜ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ë”°ë¥´ë©°, ImageNet ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤.
    - OpenImagesì—ì„œ ì‚¬ì „ í•™ìŠµëœ f = 4 ì˜¤í† ì¸ì½”ë”© ëª¨ë¸(VQ-regularization, í‘œ 8 ì°¸ì¡°)ì„ ì‚¬ìš©í•˜ê³ , ì €í•´ìƒë„ ì»¨ë””ì…”ë‹ ì •ë³´(y)ì™€ UNetì˜ ì…ë ¥ì„ ì—°ê²°í•©ë‹ˆë‹¤.
- **ê²°ê³¼**:
    - ì •ì„±ì (qualitative) ë° ì •ëŸ‰ì (quantitative) ê²°ê³¼ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ í™•ì¸
    - LDM-SR ëª¨ë¸ì€ FID(Frechet Inception Distance) ì ìˆ˜ì—ì„œ SR3ë³´ë‹¤ ìš°ìˆ˜í•˜ì§€ë§Œ, SR3ëŠ” IS(Inception Score)ì—ì„œ ë” ìš°ìˆ˜
    - ë‹¨ìˆœ ì´ë¯¸ì§€ íšŒê·€ ëª¨ë¸ì€ ê°€ì¥ ë†’ì€ PSNR(í”¼í¬ ì‹ í˜¸ ëŒ€ ì¡ìŒë¹„) ë° SSIM(êµ¬ì¡°ì  ìœ ì‚¬ì„± ì§€ìˆ˜) ì ìˆ˜ë¥¼ ë‹¬ì„±í•˜ì§€ë§Œ, ì´ëŸ¬í•œ ì§€í‘œëŠ” ì‚¬ëŒì˜ ì¸ì‹ê³¼ ì˜ ë§ì§€ ì•Šìœ¼ë©°, ê³ ì£¼íŒŒ ë””í…Œì¼ì´ ë¶ˆì™„ì „í•˜ê²Œ ì •ë ¬ëœ ê²ƒë³´ë‹¤ íë¦¿í•¨ì„ ì„ í˜¸í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤[72].

![Table 5. Ã—4 upscaling results on ImageNet-Val. (2562); â€ : FID features computed on validation split, â€¡: FID features computed on train split; âˆ—: Assessed on a NVIDIA A100](Untitled%2015.png)

Table 5. Ã—4 upscaling results on ImageNet-Val. (2562); â€ : FID features computed on validation split, â€¡: FID features computed on train split; âˆ—: Assessed on a NVIDIA A100

![Table6. Assessing inpainting efficiency.â€ :Deviations from Fig.7 due to varying GPU settings/batch sizes *cf* . the supplement.](/assets/Images/2024-05-17-stable_diffusion//assets/Images/2024-05-17-stable_diffusion/Untitled%2016.png)

Table6. Assessing inpainting efficiency.â€ :Deviations from Fig.7 due to varying GPU settings/batch sizes *cf* . the supplement.

	

**D.6. Super-Resolution :** Appendix ì— ì¶”ê°€ë¡œ ìˆìŒ

<br>

## **Inpainting with Latent Diffusion**

**E.2.2 Inpainting :** Appendix ì— ì¶”ê°€ë¡œ ìˆìŒ

![Untitled](/assets/Images/2024-05-17-stable_diffusion/Untitled%2017.png)

![Untitled](/assets/Images/2024-05-17-stable_diffusion/Untitled%2018.png)

<br>

# **Limitations & Societal Impact**

ìˆœì°¨ ìƒ˜í”Œë§ í”„ë¡œì„¸ìŠ¤ëŠ” ì—¬ì „íˆ GANë³´ë‹¤ ëŠë¦¼

ë†’ì€ ì •ë°€ë„ê°€ ìš”êµ¬ë˜ëŠ” ê²½ìš° LDMì˜ ì‚¬ìš©ì— ì˜ë¬¸

<br>

# **Appendix**

**D.4. Class-Conditional Image Synthesis on ImageNet**

![Untitled](/assets/Images/2024-05-17-stable_diffusion/Untitled%2019.png)

Table10. Comparison of a class-conditional ImageNet *LDM* with recent state-of-the-art methods for class-conditional image generation on the ImageNet [12] dataset.âˆ—: Classifier rejection sampling with the given rejection rate as proposed in [67].

**D.5. Sample Quality vs. V100 Days (Continued from Sec. 4.1)**

![Untitled](/assets/Images/2024-05-17-stable_diffusion/Untitled%2020.png)

<br>

## **E. Implementation Details and Hyperparameters**

### Hyperparameters

![Untitled](/assets/Images/2024-05-17-stable_diffusion/Untitled%2021.png)

Table12. Hyper parameters for the unconditional *LDMs* producing the numbers shown in Tab.1. All models trained on a single NVIDIA A100.

![Table13. Hyper parameters for the conditional *LDMs* trained on the ImageNet dataset for the analysis in Sec.4.1. All models trained on a single NVIDIA A100.](/assets/Images/2024-05-17-stable_diffusion/Untitled%2022.png)

Table13. Hyper parameters for the conditional *LDMs* trained on the ImageNet dataset for the analysis in Sec.4.1. All models trained on a single NVIDIA A100.

<br>

### **E.2. Implementation Details**

**Implementations of** $Ï„_Î¸$ **for conditional *LDMs***

![Table14. Hyperparameters for the unconditional *LDMs* trained on the CelebA dataset for the analysis in Fig.7. All models trained on a single NVIDIA A100. âˆ—: All models are trained for 500k iterations. If converging earlier, we used the best checkpoint for assessing the provided FID scores.](/assets/Images/2024-05-17-stable_diffusion/Untitled%2023.png)

Table14. Hyperparameters for the unconditional *LDMs* trained on the CelebA dataset for the analysis in Fig.7. All models trained on a single NVIDIA A100. âˆ—: All models are trained for 500k iterations. If converging earlier, we used the best checkpoint for assessing the provided FID scores.

![Table15. Hyperparameters for the conditional *LDMs* from Sec.4. All models trained on a single NVIDIA A100 except for the in painting model which was trained on eight V100.](/assets/Images/2024-05-17-stable_diffusion/Untitled%2024.png)

Table15. Hyperparameters for the conditional *LDMs* from Sec.4. All models trained on a single NVIDIA A100 except for the in painting model which was trained on eight V100.

$$
\zeta \leftarrow \text{TokEmb}(y) + \text{PosEmb}(y) \ \ \ \ \ \ \text{for } i = 1, \ldots, N \text{ :} \\\quad \zeta_1 \leftarrow \text{LayerNorm}(\zeta) \\\quad \zeta_2 \leftarrow \text{MultiHeadSelfAttention}(\zeta_1) + \zeta \\\quad \zeta_3 \leftarrow \text{LayerNorm}(\zeta_2) \\\quad \zeta \leftarrow \text{MLP}(\zeta_3) + \zeta_2 \\\zeta \leftarrow \text{LayerNorm}(\zeta)
$$

![Table16. Architecture of a transformer block as described in Sec.E.2.1, replacing the self-attention layer of the standard â€œablated UNetâ€ architecture [15]. Here,  $n_h$ denotes the number of attention heads and d the dimensionality per head.](/assets/Images/2024-05-17-stable_diffusion/Untitled%2025.png)

Table16. Architecture of a transformer block as described in Sec.E.2.1, replacing the self-attention layer of the standard â€œablated UNetâ€ architecture [15]. Here,  $n_h$ denotes the number of attention heads and d the dimensionality per head.

![Table17. Hyperparameters for the experiments with transformer encoders in Sec.4.3.](/assets/Images/2024-05-17-stable_diffusion/Untitled%2026.png)

Table17. Hyperparameters for the experiments with transformer encoders in Sec.4.3.