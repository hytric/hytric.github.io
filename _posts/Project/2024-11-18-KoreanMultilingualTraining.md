---
title: "[í”„ë¡œì íŠ¸] Korean Audio, Multilingual Hubert translate Training Guideline"
last_modified_at: 2024-11-18
categories:
  - Project
excerpt: "ê¸°ì¡´ Unit based audio Multilingual translateìœ¼ë¡œ ì œì•ˆëœ ë…¼ë¬¸ì— Koreanì„ ì¶”ê°€"
use_math: true
classes: wide
---

> Inha univ.  |  ê¸°ì¡´ Unit based audio Multilingual translateìœ¼ë¡œ ì œì•ˆëœ ë…¼ë¬¸ì— Koreanì„ ì¶”ê°€  
> ECE Capston design  

[ğŸˆ SG2Video challenge ì •ë¦¬í•œ í˜ì´ì§€ ë°”ë¡œê°€ê¸° ğŸˆ](https://hytric.github.io/project/KoreanMultilingualChallenges/)
> 
 
<br>


ìµœê·¼ Audio2Audio multilingual translateì—ì„œ hubertë¥¼ í™œìš©í•œ direct translationì´ ì œì•ˆë˜ì—ˆë‹¤.

[Textless Unit-to-Unit training for Many-to-Many Multilingual Speech-to-Speech Translation](https://arxiv.org/abs/2308.01831)  
[AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation with Unified Audio-Visual Speech Representation](https://arxiv.org/abs/2312.02512)


ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ì–‘í•œ ì–¸ì–´ë¥¼ ì§€ì›í•˜ì§€ë§Œ, í•œêµ­ì–´ë¥¼ ì§€ì›í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.

ê·¸ë˜ì„œ í•œê¸€ë„ ê°™ì´ ì§€ì›í•˜ë„ë¡ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤.

í˜„ì¬ Training codeê°€ ì œê³µë˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, ìì„¸í•œ ê°€ì´ë“œë¼ì¸ë„ ê°™ì´ ì œê³µí•˜ê³ ì í•œë‹¤.

![Slide10.jpg](/assets/Images/2024-11-18-KoreanMultilingualTraining/Slide10.jpg)

**fairseqë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì½”ë“œ ì‘ì„±**
 
<br>

### ê´€ë ¨ ë…¼ë¬¸ ë¶„ì„

[HuBERT](https://hytric.github.io/paperreview/HuBERT/)  
[UTUT](https://hytric.github.io/paperreview/UTUT/)  
[AV2AV](https://hytric.github.io/paperreview/AV2AV/)  
[HiFi-GAN](https://hytric.github.io/paperreview/HiFi-GAN/)  

 
<br>

## ëª©ì°¨

<aside>
ğŸ’¡

1. Hubert preprocessing  

2. Hubert training  

3. Transformer preprocessing  

4. Transformer training  

5. vocoder  
</aside>
 
<br>

## Hubert preprocessing

wave2vec manifest â†’ mmfc â†’ sample kmean

ì¶œì²˜

https://github.com/facebookresearch/fairseq/blob/main/examples/wav2vec/README.md

https://github.com/facebookresearch/fairseq/blob/main/examples/hubert/simple_kmeans/README.md

1. **manifest ìƒì„±**
    
    ì—­í•  : ì˜¤ë””ì˜¤ íŒŒì¼ì˜ ê²½ë¡œì™€ í•´ë‹¹ ê¸¸ì´ë¥¼ ëª…ì‹œí•˜ì—¬ ë°ì´í„°ì…‹ì„ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬
    
    í•™ìŠµ(training)ê³¼ ê²€ì¦(validation) ë°ì´í„° ë¶„í• 
    
    íŒŒì¼ í˜•ì‹ : `.tsv` (Tab-Separated Values) í˜•ì‹ìœ¼ë¡œ ì €ì¥
    
    - ì²« ë²ˆì§¸ ì¤„: í—¤ë” (ê²½ë¡œ, íŒŒì¼ ê¸¸ì´)
    - ì´í›„ ê° ì¤„: ê°œë³„ ì˜¤ë””ì˜¤ íŒŒì¼ì˜ ê²½ë¡œì™€ ê¸¸ì´(ì´ˆ ë‹¨ìœ„)
    
    ë…¼ë¬¸ì—ì„œ 10~30ì´ˆ ê¸¸ì´ì˜ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì‚¬ìš©
    
    Pythonì˜ `soundfile` ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (`pip install soundfile`)
    
    ```bash
    python examples/wav2vec/wav2vec_manifest.py /path/to/waves --dest /manifest/path --ext $ext --valid-percent $valid
    ```
    
    - **`/path/to/waves`**: ì˜¤ë””ì˜¤ íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬.
    - **`-dest /manifest/path`**: ìƒì„±ëœ manifest íŒŒì¼ì„ ì €ì¥í•  ê²½ë¡œ.
    - **`-ext $ext`**: ì˜¤ë””ì˜¤ íŒŒì¼ì˜ í™•ì¥ì (ì˜ˆ: wav, flac ë“±).
    - **`-valid-percent $valid`**: í•™ìŠµ ë°ì´í„°ì—ì„œ ê²€ì¦ ë°ì´í„°ë¡œ ì‚¬ìš©í•  ë¹„ìœ¨ (ì˜ˆ: 0.01 = 1%).

1. **mmfc ìƒì„±**
    
    ì—­í•  : MFCCëŠ” ì‚¬ëŒì˜ ì²­ê° ê°ê°ê³¼ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ ìŒì„± ì‹ í˜¸ë¥¼ ë¶„ì„í•´, ì¤‘ìš”í•œ íŠ¹ì§•ì„ ì €ì°¨ì›ì ìœ¼ë¡œ ìš”ì•½
    
    ì£¼íŒŒìˆ˜ ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í‘œí˜„, ìŒì„±ì˜ íƒ€ì´ë°, ìŒì¡°, ìŒì§ˆê³¼ ê°™ì€ ì¤‘ìš”í•œ ì •ë³´ë¥¼ í¬í•¨
    
    1. **Pre-emphasis**: ì‹ í˜¸ì˜ ê³ ì£¼íŒŒ ì„±ë¶„ì„ ê°•í™”, ì¡ìŒì„ ì¤„ì´ê³  ì‹ í˜¸ì˜ ì •ë³´ ê· ë“±
        - ì‹ í˜¸ì˜ ê³ ì£¼íŒŒ ì„±ë¶„ì„ ê°•í™”í•˜ë©´, ì¡ìŒì´ ëŠ˜ì–´ë‚˜ëŠ”ê±° ì•„ë‹Œê°€?
            - ê³ ì£¼íŒŒ ì‹ í˜¸ì™€ ì¡ìŒì„ ë™ì¼í•œ ë¹„ìœ¨ë¡œ ì¦ê°€
            - ê³ ì£¼íŒŒ ì„±ë¶„ì—ëŠ” ë°œìŒ êµ¬ë³„ì— ì¤‘ìš”í•œ ì •ë³´(ììŒ ë“±)ê°€ í¬í•¨
            - ìŒì„± ì‹ í˜¸ëŠ” ì¼ë°˜ì ìœ¼ë¡œ 4~8kHzì˜ ëŒ€ì—­í­, ë…¸ì´ì¦ˆëŠ” ì´ê²ƒë³´ë‹¤ ë” í¼
    2. **Framing**: ì‹ í˜¸ ë¶„í• , ì¼ë°˜ì ìœ¼ë¡œ í”„ë ˆì„ í¬ê¸°ëŠ” 20-40ms
    3. **Windowing**: ê° í”„ë ˆì„ì— ìœˆë„ìš° í•¨ìˆ˜ë¥¼ ì ìš© â†’ ì‹ í˜¸ì˜ ê²½ê³„ë¥¼ softí•˜ê²Œ ë§Œë“¬(ì£¼ë¡œ Hamming ìœˆë„ìš° ì‚¬ìš©)
    4. **Fourier Transform**: ê° í”„ë ˆì„ì— ëŒ€í•´ FFT(Fast Fourier Transform)ë¥¼ ìˆ˜í–‰, ì£¼íŒŒìˆ˜ ìŠ¤í™íŠ¸ëŸ¼ì„ ê³„ì‚°
    5. **Mel Filter Bank**: ì¸ê°„ì˜ ì²­ê° ê°ê°ì— ê¸°ë°˜í•˜ì—¬ Mel scaleë¡œ fliterë¥¼ ì ìš©
        - ì €ì£¼íŒŒëŠ” ìì„¸íˆ, ê³ ì£¼íŒŒëŠ” ëœ ìì„¸íˆ í‘œí˜„.
    6. **Logarithm**: Mel í•„í„° ì¶œë ¥ì— ë¡œê·¸ë¥¼ ì·¨í•˜ì—¬ ìŒì„± ì‹ í˜¸ì˜ í¬ê¸° ì°¨ì´ë¥¼ ì¤„ì„
    7. **Discrete Cosine Transform (DCT)**: ë¡œê·¸ ìŠ¤í™íŠ¸ëŸ¼ì„ ì‹œê°„ ì˜ì—­ìœ¼ë¡œ ë³€í™˜ â†’ MFCC ë²¡í„°ë¥¼ ìƒì„± , ë³´í†µ ìƒìœ„ 12~13ê°œì˜ ê³„ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©°, ì´ë¡œë¶€í„° **delta**(ì†ë„ ì •ë³´)ì™€ **delta-delta**(ê°€ì†ë„ ì •ë³´) ê³„ìˆ˜ë¥¼ ì¶”ê°€ë¡œ ê³„ì‚°
    
    `.npy` ë˜ëŠ” `.len` í˜•ì‹ìœ¼ë¡œ ì €ì¥
    
    ```bash
    python dump_mfcc_feature.py ${tsv_dir} ${split} ${nshard} ${rank} ${feat_dir}
    ```
    
    - **`${tsv_dir}`**: ë°ì´í„°ì…‹ manifest íŒŒì¼ì˜ ë””ë ‰í† ë¦¬.
    - **`${split}`**: ë°ì´í„°ë¥¼ ë¶„ë¦¬í•œ ì¢…ë¥˜(e.g., train, valid).
    - **`${nshard}`**: ì „ì²´ ë°ì´í„°ë¥¼ ëª‡ ê°œì˜ ìƒ¤ë“œ(shard)ë¡œ ë‚˜ëˆŒì§€ ì„¤ì •.
    - **`${rank}`**: ì²˜ë¦¬í•  ìƒ¤ë“œ ë²ˆí˜¸ (0ë¶€í„° ì‹œì‘).
    
2. **K-means clustering**
    
    featureë¥¼ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ K-means í´ëŸ¬ìŠ¤í„°ë§ì„ ì‹¤í–‰
    
    ```bash
    python learn_kmeans.py ${feat_dir} ${split} ${nshard} ${km_path} ${n_clusters} --percent 0.1
    ```
    
    ### **íŒŒë¼ë¯¸í„° ì„¤ëª…**
    
    - **`${feat_dir}`**: ì¶”ì¶œëœ íŠ¹ì§•(feature)ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬.
    - **`${split}`**: ë°ì´í„° ë¶„í•  ì´ë¦„ (e.g., train, valid).
    - **`${nshard}`**: ë°ì´í„°ì…‹ì„ ë‚˜ëˆ„ëŠ” ìƒ¤ë“œ ê°œìˆ˜.
    - **`${km_path}`**: í•™ìŠµëœ K-means ëª¨ë¸ì´ ì €ì¥ë  ê²½ë¡œ.
    - **`${n_clusters}`**: K-means í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ (e.g., 100, 500, 1000).
    - **`-percent`**:
        - ì‚¬ìš©í•  ë°ì´í„°ì˜ ë¹„ìœ¨. 10% ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ê²½ìš° `0.1`.
        - `1`ë¡œ ì„¤ì •í•˜ë©´ ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©.
    
3. **K-means application**
    
    í•™ìŠµëœ K-means ëª¨ë¸ì„ ì‚¬ìš©í•´ íŠ¹ì§• ë°ì´í„°ì— í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸” í• ë‹¹
    
    ```bash
    python dump_km_label.py ${feat_dir} ${split} ${km_path} ${nshard} ${rank} ${lab_dir}
    ```
    
    - **`${feat_dir}`**: íŠ¹ì§•(feature)ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬.
    - **`${split}`**: ë°ì´í„° ë¶„í•  ì´ë¦„ (e.g., train, valid).
    - **`${km_path}`**: í•™ìŠµëœ K-means ëª¨ë¸ ê²½ë¡œ.
    - **`${nshard}`**: ìƒ¤ë“œ ê°œìˆ˜.
    - **`${rank}`**: ì²˜ë¦¬í•  ìƒ¤ë“œì˜ ìˆœë²ˆ (0ë¶€í„° ì‹œì‘).
    - **`${lab_dir}`**: ë ˆì´ë¸”ì´ ì €ì¥ë  ë””ë ‰í† ë¦¬
    
    `${lab_dir}/${split}_${rank}_${shard}.km` í˜•ì‹ìœ¼ë¡œ ì €ì¥
    
    ìƒ¤ë“œë³„ë¡œ ì €ì¥ëœ ë ˆì´ë¸” íŒŒì¼ì„ í•˜ë‚˜ë¡œ ë³‘í•©.
    
    ```bash
    for rank in $(seq 0 $((nshard - 1))); do
      cat $lab_dir/${split}_${rank}_${nshard}.km
    done > $lab_dir/${split}.km
    ```
    
     `${lab_dir}/${split}.km`
    

5. **Create a dummy dict**
    
    ë ˆì´ë¸”ê³¼ ê°€ì¤‘ì¹˜ë¥¼ HuBERT í›ˆë ¨ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì‚¬ì „ í˜•ì‹ìœ¼ë¡œ ì €ì¥
    
    ```bash
    for x in $(seq 0 $((n_clusters - 1))); do
      echo "$x 1"
    done >> $lab_dir/dict.km.txt
    ```
    
    ë”ë¯¸ ì‚¬ì „ íŒŒì¼ `${lab_dir}/dict.km.txt`
     
<br>


## Hubert training

í•™ìŠµì„ ìœ„í•´ í•„ìš”í•œ ë°ì´í„° ëª©ë¡

 `{train, valid}.tsv` : **manifest** íŒŒì¼

`{train, valid}.km` : **K-means application** íŒŒì¼

`dict.km.txt`: **dummy dict** íŒŒì¼

hubert pre-train

```bash
$ python fairseq_cli/hydra_train.py \
  --config-dir /path/to/fairseq-py/examples/hubert/config/pretrain \
  --config-name hubert_base_librispeech \
  task.data=/path/to/data task.label_dir=/path/to/labels task.labels='["km"]' model.label_rate=100
```

hubert fine-tuning

```bash
$ python fairseq_cli/hydra_train.py \
  --config-dir /path/to/fairseq-py/examples/hubert/config/finetune \
  --config-name base_10h \
  task.data=/path/to/data task.label_dir=/path/to/trans \
  model.w2v_path=/path/to/checkpoint
```
 
<br>

## Transformer í•™ìŠµì„ ìœ„í•œ  Data preprocessing

<aside>
ğŸ’¡

í•„ìš”í•œ ë°ì´í„° : .wav íŒŒì¼(ì–¸ì–´ë³„ë¡œ íŒŒì¼ ë¶„ë¥˜) , ì‚¬ì „ì— í•™ìŠµëœ hubert ëª¨ë¸ , KM íŒŒì¼

</aside>

1. **Hubert inference ëŒë¦¬ê¸°**

ì´ì „ì— í•™ìŠµëœ Hubert ëª¨ë¸ì„ ê°€ì§€ê³  Multilingual datasetì„ Unitìœ¼ë¡œ ë³€ê²½

[inference.py](/assets/Images/2024-11-18-KoreanMultilingualTraining/inference.py)

[util.py](/assets/Images/2024-11-18-KoreanMultilingualTraining/util.py)

ìœ„ 2ê°œ íŒŒì¼ fairseq í´ë”ì— ë„£ê¸°

```bash
cd fairseq
python inference.py \
	--in-wav-path <wavíŒŒì¼ê²½ë¡œ> \
	--out-unit-path <unitì¶œë ¥ì €ì¥ê²½ë¡œ> \
	--mhubert-path <ëª¨ë¸íŒŒë¼ë¯¸í„°ê²½ë¡œ> \
	--kmeans-path <k-meansíŒŒì¼ê²½ë¡œ>
```

1. **ë°ì´í„° íŒŒì¼ ì•ˆì— ë‚´ìš© txt íŒŒì¼ë¡œ ì €ì¥**

íŒŒì¼ì´ í•´ë‹¹ Unit ë°ì´í„°ë¥¼ ì‰½ê²Œ ì°¾ì„ ìˆ˜ ìˆë„ë¡ í•¨

```bash
find /home/jskim/audio/dataset/units/en/ -maxdepth 1 -name '*.unit' | sort > en_files.txt
find /home/jskim/audio/dataset/units/es/ -maxdepth 1 -name '*.unit' | sort > es_files.txt
find /home/jskim/audio/dataset/units/fr/ -maxdepth 1 -name '*.unit' | sort > fr_files.txt
```

1. **TR í•™ìŠµ ë°ì´í„° ì¤€ë¹„**

ê° ì–¸ì–´ ìŒì— ëŒ€í•´ í•™ìŠµ ë° ê²€ì¦ ë°ì´í„°ë¥¼ ìƒì„±, Fairseqì—ì„œ ìš”êµ¬í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ì¤€ë¹„

```bash
dataset/
â””â”€â”€ units/
    â”œâ”€â”€ en/
    â”œâ”€â”€ es/
    â””â”€â”€ fr/
```

ë‹¤ìŒê³¼ ê°™ì´ íŒŒì¼ êµ¬ì¡°ë¥¼ ë§Œë“¤ê¸°

[unit_txt_gen.py](/assets/Images/2024-11-18-KoreanMultilingualTraining/unit_txt_gen.py)

[unit_TR_preprocessing.py](/assets/Images/2024-11-18-KoreanMultilingualTraining/unit_TR_preprocessing.py)

ê° ì–¸ì–´ì˜ íŒŒì¼ ìˆ˜ê°€ ë™ì¼í•œì§€ í™•ì¸ â†’ ***parallel corpus*** ë§ì¶”ê¸°

í•™ìŠµê³¼ ê²€ì¦ ë°ì´í„°ë¡œ ë°ì´í„°ë¥¼ ë¶„í• . ì¼ë°˜ì ìœ¼ë¡œ 90%ë¥¼ í•™ìŠµìš©ìœ¼ë¡œ, 10%ë¥¼ ê²€ì¦ìš©ìœ¼ë¡œ ì‚¬ìš©

```bash
# ëª…ë ¹í–‰ ì¸ì íŒŒì„œ ì„¤ì •
parser = argparse.ArgumentParser(description='ë‹¤êµ­ì–´ ë°ì´í„° ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸')
parser.add_argument('--base_dir', type=str, default='/home/jskim/audio/dataset/units',
                    help='ì…ë ¥ ë°ì´í„°ì˜ ê¸°ë³¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ')
parser.add_argument('--output_base_dir', type=str, default='/home/jskim/audio/dataset/units',
                    help='ì¶œë ¥ ë°ì´í„°ê°€ ì €ì¥ë  ê¸°ë³¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ')
parser.add_argument('--languages', type=str, nargs='+', default=['en', 'es', 'fr'],
                    help='ì²˜ë¦¬í•  ì–¸ì–´ ëª©ë¡ (ì˜ˆ: en es fr)')
parser.add_argument('--train_ratio', type=float, default=0.9,
                    help='í•™ìŠµ ë°ì´í„° ë¹„ìœ¨ (0ê³¼ 1 ì‚¬ì´ì˜ ì‹¤ìˆ˜)')
parser.add_argument('--random_seed', type=int, default=42,
                    help='ëœë¤ ì‹œë“œ ê°’')
args = parser.parse_args()
```

ë‹¤ìŒ ì½”ë“œë¡œ ë™ì¼í•œ ***parallel corpus*** ë¥¼ ì°¾ì„ ìˆ˜ ìˆë„ë¡ í•´ì•¼í•¨

```bash
filename_pattern = re.compile(r'(.+)_([a-z]{2})_(\d+)\.unit$')
```

ìœ„ íŒŒì¼ ì‹¤í–‰ í–ˆì„ ë•Œ 4ê°œ txt íŒŒì¼ì´ ë‚˜ì™€ì•¼í•¨

train.src, train.tgt, valid.src, valid.tgt

1. fairseq Data Preprocessing

```bash
fairseq-preprocess \
  --source-lang src \
  --target-lang tgt \
  --trainpref "/path/to/train" \
  --validpref "/path/to/valid" \
  --destdir "/path/to/data-bin" \
  --workers 4 \
  --joined-dictionary \
  --dataset-impl 'mmap'
```

ìœ„ ì½”ë“œë¥¼ í†µí•´ ì‹¤ì œ TRì— ë“¤ì–´ê°€ëŠ” data êµ¬ì¡° ì™„ì„± 

dict íŒŒì¼ë„ ê°™ì´ ë“¤ì–´ê°

ì—¬ê¸°ì—ì„œëŠ” í•˜ë‚˜ì˜ multilingual Hubertê°€ ë“¤ì–´ê°€ê¸° ë•Œë¬¸ì— 1ê°œì˜ dictë§Œ í•„ìš”í•¨

### Trasformer Training code

```bash
fairseq-train '/shared/home/milab/users/jskim/multilingual' \
  --arch transformer \
  --share-decoder-input-output-embed \
  --encoder-layers 12 \
  --decoder-layers 12 \
  --encoder-embed-dim 512 \
  --decoder-embed-dim 512 \
  --encoder-attention-heads 8 \
  --decoder-attention-heads 8 \
  --encoder-ffn-embed-dim 2048 \
  --decoder-ffn-embed-dim 2048 \
  --max-tokens 24000 \ 
  --update-freq 2 \     
  --optimizer adam \
  --adam-betas '(0.9, 0.98)' \
  --lr 5e-4 \
  --lr-scheduler inverse_sqrt \
  --warmup-updates 4000 \
  --dropout 0.3 \
  --weight-decay 0.0001 \
  --criterion label_smoothed_cross_entropy \
  --label-smoothing 0.1 \
  --max-epoch 50 \
  --save-dir '/shared/home/milab/users/jskim/results' \
  --patience 10 \
  --tensorboard-logdir '/shared/home/milab/users/jskim/tensorboard_logs' \
  --max-source-positions 3000 \
  --max-target-positions 3000 \
  --amp
```
 
<br>

## Vocoder

https://github.com/facebookresearch/speech-resynthesis/tree/main/examples/speech_to_speech_translation

í•™ìŠµì„ ìœ„í•´ í•„ìš”í•œ ë°ì´í„° ëª©ë¡

HuBERT ë‹¨ìœ„ íŒŒì¼ (e.g., `.txt` )

ì˜¤ë””ì˜¤ íŒŒì¼ `.wav` ë˜ëŠ” `.flac`

ë©”íƒ€ë°ì´í„° íŒŒì¼(`.tsv`)
 
<br>

**JSON íŒŒì¼ ì˜ˆì œ**

```json
json
ì½”ë“œ ë³µì‚¬
{
  "data": {
    "train_wav": "/path/to/train/audio/",
    "train_units": "/path/to/train/units/",
    "train_durations": "/path/to/train/durations/",
    "valid_wav": "/path/to/valid/audio/",
    "valid_units": "/path/to/valid/units/",
    "valid_durations": "/path/to/valid/durations/"
  },
  "training": {
    "batch_size": 32,
    "num_epochs": 100,
    "learning_rate": 0.0002
  },
  "model": {
    "n_units": 1000,
    "hop_length": 256,
    "sampling_rate": 16000
  }
}

```
 
<br>

**Training code**

```bash
python -m torch.distributed.launch --nproc_per_node <NUM_GPUS> \
    -m examples.speech_to_speech_translation.train \
    --checkpoint_path checkpoints/lj_hubert100_dur1.0 \
    --config examples/speech_to_speech_translation/configs/hubert100_dw1.0.json
```
 
<br>

**Inference**

```bash
python -m examples.speech_to_speech_translation.inference \
    --checkpoint_file checkpoints/lj_hubert100_dur1.0 \
    -n 10 \
    --output_dir generations \
    --num-gpu <NUM_GPUS> \
    --input_code_file ./datasets/LJSpeech/hubert100/val.txt \
    --dur-prediction
```