from torch.utils.data import Dataset
import torchvision.transforms as transforms
import h5py
import PIL
import os
import random
import numpy as np
import json
import torch
from datasets.transform import VGResize
from utils import imagenet_preprocess
from global_var import *
import matplotlib.pyplot as plt
import pandas as pd
from itertools import chain
from collections import Counter
from collections import deque

class AgSceneGraphDataset(Dataset):
    def __init__(self, vocab, h5_path, image_dir, image_size=(256, 256), normalize_images=True, max_objects=10, max_samples=None,
                 include_relationships=True, use_orphaned_objects=True):
        super(AgSceneGraphDataset, self).__init__()

        self.image_dir = image_dir
        self.image_size = image_size
        self.vocab = vocab
        self.num_objects = len(vocab['object_idx_to_name'])
        self.num_attributes = len(vocab['attribute_idx_to_name'])
        self.use_orphaned_objects = use_orphaned_objects
        self.max_objects = max_objects
        self.max_samples = max_samples
        self.include_relationships = include_relationships

        transform = [VGResize(image_size), transforms.ToTensor()]

        transform.append(imagenet_preprocess())
        self.transform = transforms.Compose(transform)

        self.data = {}
        with h5py.File(h5_path, 'r') as f:
            for k, v in f.items():
                if k == 'image_paths':
                    self.image_paths = list(v)
                else:
                    self.data[k] = torch.IntTensor(np.asarray(v))
                    
        if 'train' in h5_path:
            file_path = '/home/jskim/Graph_VIdeo_generation/dataset/action-genome/train_id_to_image.json'
        else:
            file_path = '/home/jskim/Graph_VIdeo_generation/dataset/action-genome/test_id_to_image.json'
        
        with open(file_path, 'r') as file:
            data = json.load(file)
        self.videotoid = data
        self.keys_list = list(self.videotoid.keys())
        
        # 모든 프레임 한번에
        # self.all_frame = []
        # for i in self.keys_list:
        #     self.all_frame.extend(self.videotoid[i])
        
        # 앞선 프레임 부터 순차적으로
        self.videotoid_queues = {video_id: deque(self.videotoid[video_id]) for video_id in self.keys_list}
        

    def __len__(self):
        # 모든 프레임 한번에
        # return len(self.all_frame)
    
        # 앞선 프레임 부터 순차적으로 
        return len(self.keys_list)

    def __getitem__(self, index):
        # 모든 프레임 한번에
        # index = self.all_frame[index]
        # img_path = os.path.join(self.image_dir, str(self.image_paths[index], encoding="utf-8"))
        
        # 앞선 프레임 부터 순차적으로 
        video_id = self.keys_list[index]
        index = self.videotoid_queues[video_id].popleft()
        self.videotoid_queues[video_id].append(index)
        img_path = os.path.join(self.image_dir, str(self.image_paths[index], encoding="utf-8"))
        index = frame_id
        
        with open(img_path, 'rb') as f:
            with PIL.Image.open(f) as image:
                WW, HH = image.size
                image = self.transform(image.convert('RGB'))

        H, W = self.image_size

        obj_idxs_with_rels = set()
        obj_idxs_without_rels = set(range(self.data['objects_per_image'][index].item()))
        for r_idx in range(self.data['relationships_per_image'][index]):
            s = self.data['relationship_subjects'][index, r_idx].item()
            o = self.data['relationship_objects'][index, r_idx].item()
            obj_idxs_with_rels.add(s)
            obj_idxs_with_rels.add(o)
            obj_idxs_without_rels.discard(s)
            obj_idxs_without_rels.discard(o)

        obj_idxs = list(obj_idxs_with_rels)
        obj_idxs_without_rels = list(obj_idxs_without_rels)
        if len(obj_idxs) > self.max_objects - 1:
            obj_idxs = random.sample(obj_idxs, self.max_objects)
        if len(obj_idxs) < self.max_objects - 1 and self.use_orphaned_objects:
            num_to_add = self.max_objects - 1 - len(obj_idxs)
            num_to_add = min(num_to_add, len(obj_idxs_without_rels))
            obj_idxs += random.sample(obj_idxs_without_rels, num_to_add)
        O = len(obj_idxs) + 1

        objs = torch.LongTensor(O).fill_(-1)

        attributes = torch.LongTensor(O).fill_(-1)
        boxes = torch.FloatTensor([[0, 0, 1, 1]]).repeat(O, 1)
        obj_idx_mapping = {}
        for i, obj_idx in enumerate(obj_idxs):
            objs[i] = self.data['object_names'][index, obj_idx].item()
            x, y, w, h = self.data['object_boxes'][index, obj_idx].tolist()
            x0 = float(x) / WW
            y0 = float(y) / HH
            x1 = float(x + w) / WW
            y1 = float(y + h) / HH
            boxes[i] = torch.FloatTensor([x0, y0, x1, y1])
            obj_idx_mapping[obj_idx] = i
            attributes[i] = self.data['attributes_per_object'][index, obj_idx].item()

        objs[O - 1] = self.vocab['object_name_to_idx']['__image__']

        triples = []
        for r_idx in range(self.data['relationships_per_image'][index].item()):
            if not self.include_relationships:
                break
            s = self.data['relationship_subjects'][index, r_idx].item()
            p = self.data['relationship_predicates'][index, r_idx].item()
            o = self.data['relationship_objects'][index, r_idx].item()
            s = obj_idx_mapping.get(s, None)
            o = obj_idx_mapping.get(o, None)
            if s is not None and o is not None:
                triples.append([s, p, o])

        in_image = self.vocab['pred_name_to_idx']['__in_image__']
        for i in range(O - 1):
            triples.append([i, in_image, O - 1])

        triples = torch.LongTensor(triples)
        return image, objs, boxes, triples, attributes


def ag_collate_fn(batch):
    all_imgs, all_objs, all_boxes, all_triples, all_attributes = [], [], [], [], []
    all_obj_to_img, all_triple_to_img = [], []
    obj_offset = 0
    for i, (img, objs, boxes, triples, attributes) in enumerate(batch):
        all_imgs.append(img[None])
        O, T = objs.size(0), triples.size(0)
        all_objs.append(objs)
        all_boxes.append(boxes)
        triples = triples.clone()
        triples[:, 0] += obj_offset
        triples[:, 2] += obj_offset
        all_triples.append(triples)

        all_obj_to_img.append(torch.LongTensor(O).fill_(i))
        all_triple_to_img.append(torch.LongTensor(T).fill_(i))
        obj_offset += O

        all_attributes.append(attributes)

    all_imgs = torch.cat(all_imgs)
    all_objs = torch.cat(all_objs)
    all_boxes = torch.cat(all_boxes)
    all_triples = torch.cat(all_triples)
    all_obj_to_img = torch.cat(all_obj_to_img)
    all_triple_to_img = torch.cat(all_triple_to_img)

    out = (all_imgs, all_objs, all_boxes, all_triples, all_obj_to_img, all_triple_to_img)
    return out


def build_ag_dsets(args):
    with open(args.vocab_json, 'r') as f:
        vocab = json.load(f)
    dset_kwargs = {
        'vocab': vocab,
        'h5_path': args.train_h5,
        'image_dir': args.train_data,
        'image_size': (args.image_size, args.image_size),
        'max_samples': None,
        'max_objects': args.max_objects_per_image,
        'use_orphaned_objects': args.use_orphaned_objects,
        'include_relationships': args.include_relationships,
    }
    train_dset = AgSceneGraphDataset(**dset_kwargs)
    iter_per_epoch = len(train_dset) // args.batch_size
    print('There are %d iterations per epoch' % iter_per_epoch)

    dset_kwargs['h5_path'] = args.val_h5
    del dset_kwargs['max_samples']
    val_dset = AgSceneGraphDataset(**dset_kwargs)

    return vocab, train_dset, val_dset